{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../lib\")\n",
    "from load import mnist\n",
    "from load import faces\n",
    "import pickle\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.downsample import max_pool_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentDir =  os.getcwd();\n",
    "parampickle = currentDir + \"/parametersConvonet.pickle\"\n",
    "logPickle = currentDir + \"/logPickleDBN.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trX, teX, trY, teY = mnist(onehot=True)\n",
    "\n",
    "# trX = trX.reshape(-1, 1, 28, 28)\n",
    "# teX = teX.reshape(-1, 1, 28, 28)\n",
    "\n",
    "trX, teX, trY, teY = faces(onehot=True)\n",
    "\n",
    "trX = trX.reshape(-1, 1, 48, 48)\n",
    "teX = teX.reshape(-1, 1, 48, 48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    W = theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "    return W\n",
    "\n",
    "def init_bias(shape):\n",
    "    b_values = np.zeros((shape[0],), dtype=theano.config.floatX)\n",
    "    b = theano.shared(value=b_values, borrow=True )\n",
    "    return b\n",
    "\n",
    "def rectify(X):\n",
    "    return T.maximum(X, 0.)\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X *  srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X = (X/retain_prob)\n",
    "    return X\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def negative_log_likelihood(p_y_given_x,y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X, w, b , w2, b2 ,  w3, b3, w4, b4, w5 , b5, w6 , b6 , w7 , b7 , w_o, b_o, p_drop_conv, p_drop_hidden):\n",
    "    l1a = conv2d(X, w)\n",
    "    l1a = rectify(l1a + b.dimshuffle('x', 0, 'x', 'x'))\n",
    "    l1 = max_pool_2d(l1a, (2, 2))\n",
    "    l1 = dropout(l1, p_drop_conv)\n",
    "\n",
    "    l2a = conv2d(l1, w2)\n",
    "    l2a = rectify(l2a + b2.dimshuffle('x', 0, 'x', 'x'))\n",
    "    l2 = max_pool_2d(l2a, (2, 2))\n",
    "    l2 = dropout(l2, p_drop_conv)\n",
    "    \n",
    "    l3a = conv2d(l2, w3)\n",
    "    l3a = rectify(l3a + b3.dimshuffle('x', 0, 'x', 'x'))\n",
    "    #l3 = max_pool_2d(l3a, (2, 2))\n",
    "    l3 = dropout(l3a, p_drop_conv)\n",
    "\n",
    "    l4a = conv2d(l3, w4)\n",
    "    l4b = rectify(l4a + b4.dimshuffle('x', 0, 'x', 'x'))\n",
    "    l4 = T.flatten(l4b, outdim=2)\n",
    "    l4 = dropout(l4, p_drop_conv)\n",
    "\n",
    "    l5 = rectify(T.dot(l4, w5) + b5.dimshuffle('x', 0 ))\n",
    "    \n",
    "    l6 = rectify(T.dot(l5, w6) + b6.dimshuffle('x', 0))\n",
    "    \n",
    "    l7 = rectify(T.dot(l6, w7) + b7.dimshuffle('x', 0))\n",
    "\n",
    "    pyx = T.nnet.softmax(T.dot(l7, w_o) + b_o.dimshuffle('x', 0))\n",
    "    return l1, l2, l3, l4, l5, l6, l7 , pyx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "X = T.ftensor4()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((32, 1, 7, 7))\n",
    "b = init_bias((32,1,1,1))\n",
    "\n",
    "w2 = init_weights((64, 32, 5, 5))\n",
    "b2 = init_bias((64,1,1,1))\n",
    "\n",
    "w3 = init_weights((128, 64, 3, 3))\n",
    "b3 = init_bias((128,1,1,1))\n",
    "\n",
    "w4 = init_weights((256, 128, 3, 3))\n",
    "b4 = init_bias((256,1,1,1))\n",
    "\n",
    "w5 = init_weights((256 * 5 * 5, 10000)) \n",
    "b5 = init_bias((10000,1))\n",
    "\n",
    "w6 = init_weights((10000, 1000)) \n",
    "b6 = init_bias((1000 , 1))\n",
    "\n",
    "w7 = init_weights((1000, 1000)) \n",
    "b7 = init_bias((1000 , 1))\n",
    "\n",
    "w_o = init_weights((1000, 7))\n",
    "b_o = init_bias((7,1))\n",
    "\n",
    "#Train Loop\n",
    "noise_l1, noise_l2, noise_l3, noise_l4, noise_l5 , noise_l6, noise_l7, noise_py_x = model(X, w, b, w2, b2, w3, b3, w4, b4, w5, b5, w6, b6, w7, b7, w_o, b_o, 0.2, 0.5)\n",
    "\n",
    "params = [ w, b, w2, b2, w3, b3, w4, b4, w5, b5, w6, b6, w7, b7, w_o, b_o ]\n",
    "\n",
    "cost = T.nnet.categorical_crossentropy(noise_py_x, Y).mean()\n",
    "#cost = negative_log_likelihood(noise_py_x , Y)\n",
    "#cost = T.nnet.binary_crossentropy(noise_py_x, Y).mean()\n",
    "\n",
    "#updates = RMSprop(cost, params, lr=0.0009)\n",
    "\n",
    "grads = T.grad(cost, params)\n",
    "updates = [ (param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(params, grads)]\n",
    "    \n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#Predict Loop\n",
    "l1, l2, l3, l4, l5,l6, l7, py_x = model(X, w, b, w2, b2, w3, b3, w4, b4, w5, b5, w6, b6, w7, b7, w_o, b_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(102):\n",
    "    for start, end in zip(range(0, len(trX), 500), range(500, len(trX), 500)):\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    error = np.mean(np.argmax(teY, axis=1) == predict(teX))\n",
    "    logline = \"Epoch: \" + str(i) + \"  Error: \" + str(error)\n",
    "    print logline\n",
    "    f = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f);\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion = (np.argmax(teY, axis=1) == predict(teX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = predict(teX)\n",
    "print predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real = np.argmax(teY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion = np.zeros((7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0 , len(real)):\n",
    "    j = real[i]\n",
    "    k = predicted[i]\n",
    "    #print \"Real:  %d , predicted %d\"%(j,k)\n",
    "    confusion[j][k] = confusion[j][k] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(0,7):\n",
    "    #print (confusion[j][j]/sum(confusion[j][:]))\n",
    "    print (sum(confusion[j][:]) - confusion[j][j])/sum(confusion[j][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.histogram(real , bins=[0,1, 2, 3 , 4 , 5 , 6 , 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logline = \" =======Saving Network params==== \"\n",
    "print logline\n",
    "f = open(logPickle, 'a+')\n",
    "f2 = open(parampickle , 'a+')\n",
    "pickle.dump(logline , f);\n",
    "pickle.dump(params , f2)\n",
    "f2.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = open(parampickle , 'a+')\n",
    "for p in params:\n",
    "    pickle.dump(p.get_value(),f2)\n",
    "    \n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
