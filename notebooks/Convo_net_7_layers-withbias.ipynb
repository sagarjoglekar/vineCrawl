{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../lib\")\n",
    "from load import mnist\n",
    "from load import faces\n",
    "from load import getValData\n",
    "import pickle\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.downsample import max_pool_2d\n",
    "from six.moves import cPickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "currentDir =  os.getcwd();\n",
    "logPickle = currentDir + \"/ErrosCNN7LayersV2.pickle\"\n",
    "modelDir = currentDir + \"/ModelsV2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trX, teX, trY, teY = mnist(onehot=True)\n",
    "\n",
    "# trX = trX.reshape(-1, 1, 28, 28)\n",
    "# teX = teX.reshape(-1, 1, 28, 28)\n",
    "\n",
    "trX, teX, trY, teY = faces(onehot=True)\n",
    "\n",
    "ValX , ValY = getValData()\n",
    "\n",
    "trX = trX.reshape(-1, 1, 48, 48)\n",
    "teX = teX.reshape(-1, 1, 48, 48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[467  56 496 895 653 415  95] [3995  436 4097 7215 4830 3171   49]\n"
     ]
    }
   ],
   "source": [
    "testCount = np.sum(teY , axis = 0)\n",
    "trainCount = np.sum(trY , axis = 0)\n",
    "print testCount , trainCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#re format the data and drop the neurtral images \n",
    "\n",
    "r = len(trY)\n",
    "i = 0\n",
    "while i < r:\n",
    "    if(trY[i][6] == 1):\n",
    "        trX = np.delete(trX , i , 0)\n",
    "        trY = np.delete(trY , i , 0)\n",
    "        i = i-1\n",
    "        r = len(trY)\n",
    "    else:\n",
    "        i = i+1\n",
    "\n",
    "r = len(teY) \n",
    "i = 0\n",
    "while i < r:\n",
    "    if(teY[i][6] == 1):\n",
    "        teX = np.delete(teX , i , 0)\n",
    "        teY = np.delete(teY , i , 0)\n",
    "        i = i-1\n",
    "        r = len(teY)\n",
    "    else:\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[467  56 496 895 653 415   0] [3995  436 4097 7215 4830 3171    0]\n"
     ]
    }
   ],
   "source": [
    "testCount = np.sum(teY , axis = 0)\n",
    "trainCount = np.sum(trY , axis = 0)\n",
    "print testCount , trainCount\n",
    "teY = np.delete(teY , 6 , 1)\n",
    "trY = np.delete(trY , 6 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23744, 6)\n",
      "(2982, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print trY.shape\n",
    "print teY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    W = theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "    return W\n",
    "\n",
    "def init_bias(shape):\n",
    "    b_values = np.zeros((shape[0],), dtype=theano.config.floatX)\n",
    "    b = theano.shared(value=b_values, borrow=True )\n",
    "    return b\n",
    "\n",
    "def rectify(X):\n",
    "    return T.maximum(X, 0.)\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X *  srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X = (X/retain_prob)\n",
    "    return X\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def negative_log_likelihood(p_y_given_x,y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "def saveModel(i):\n",
    "    snapshot = modelDir + \"ModelSnapshot\" + str(i) +\".pkl\"    \n",
    "    mfile = open(snapshot, 'wb')\n",
    "    cPickle.dump(params, mfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    mfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "w = init_weights((64, 1, 7, 7))\n",
    "b = init_bias((64,1,1,1))\n",
    "\n",
    "w2 = init_weights((128, 64, 5, 5))\n",
    "b2 = init_bias((128,1,1,1))\n",
    "\n",
    "w3 = init_weights((256, 128, 3, 3))\n",
    "b3 = init_bias((256,1,1,1))\n",
    "\n",
    "w4 = init_weights((256, 256, 3, 3))\n",
    "b4 = init_bias((256,1,1,1))\n",
    "\n",
    "w5 = init_weights((256 * 6 * 6, 10000)) \n",
    "b5 = init_bias((10000,1))\n",
    "\n",
    "w6 = init_weights((10000, 1000)) \n",
    "b6 = init_bias((1000 , 1))\n",
    "\n",
    "w_o = init_weights((1000, 6))\n",
    "b_o = init_bias((6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X, w, b , w2, b2 ,  w3, b3, w4, b4, w5 , b5, w6 , b6 , w_o, b_o, p_drop_conv, p_drop_hidden):\n",
    "    l1a = conv2d(X, w)\n",
    "    l1a = rectify(l1a + b.dimshuffle('x', 0, 'x', 'x'))\n",
    "    l1 = max_pool_2d(l1a, (2, 2))\n",
    "    l1 = dropout(l1, p_drop_conv)\n",
    "\n",
    "    l2a = conv2d(l1, w2)\n",
    "    l2a = rectify(l2a + b2.dimshuffle('x', 0, 'x', 'x'))\n",
    "    #l2 = max_pool_2d(l2a, (2, 2))\n",
    "    l2 = dropout(l2a, p_drop_conv)\n",
    "    \n",
    "    l3a = conv2d(l2, w3)\n",
    "    l3a = rectify(l3a + b3.dimshuffle('x', 0, 'x', 'x'))\n",
    "    l3 = max_pool_2d(l3a, (2, 2))\n",
    "    l3 = dropout(l3, p_drop_conv)\n",
    "\n",
    "    l4a = conv2d(l3, w4)\n",
    "    l4b = rectify(l4a + b4.dimshuffle('x', 0, 'x', 'x'))\n",
    "    l4 = T.flatten(l4b, outdim=2)\n",
    "    l4 = dropout(l4, p_drop_conv)\n",
    "\n",
    "    l5 = rectify(T.dot(l4, w5) + b5.dimshuffle('x', 0 ))\n",
    "    l5 = dropout(l5, p_drop_hidden)\n",
    "    \n",
    "    l6 = rectify(T.dot(l5, w6) + b6.dimshuffle('x', 0))\n",
    "    l6 = dropout(l6, p_drop_hidden)\n",
    "\n",
    "    # Add numerically stable softmax\n",
    "    #pyx = T.nnet.softmax(T.dot(l6, w_o) + b_o.dimshuffle('x', 0))\n",
    "    opVec = T.dot(l6, w_o) + b_o.dimshuffle('x', 0)\n",
    "    xdev = opVec-opVec.max(1,keepdims=True)\n",
    "    pyx = xdev - T.log(T.sum(T.exp(xdev),axis=1,keepdims=True))\n",
    "    return l1, l2, l3, l4, l5, l6, pyx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "batch_size = 150\n",
    "\n",
    "X = T.ftensor4()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "#Train Loop\n",
    "noise_l1, noise_l2, noise_l3, noise_l4, noise_l5 , noise_l6, noise_py_x = model(X, w, b, w2, b2, w3, b3, w4, b4, w5, b5, w6, b6, w_o, b_o, 0.2, 0.5)\n",
    "\n",
    "params = [ w, b, w2, b2, w3, b3, w4, b4, w5, b5, w6, b6, w_o, b_o ]\n",
    "\n",
    "#Cross Entropy error\n",
    "#cost = T.nnet.categorical_crossentropy(noise_py_x, Y).mean()\n",
    "\n",
    "#stabler Cross Entropy\n",
    "\n",
    "cost = -T.sum(Y*noise_py_x,axis=1).mean()\n",
    "\n",
    "#Mean Square Error\n",
    "#cost = T.mean(T.sum(T.sqr(noise_py_x - Y), axis=1), axis=0)\n",
    "\n",
    "#Negative log likelyhood\n",
    "#cost = negative_log_likelihood(noise_py_x , Y)\n",
    "\n",
    "#Binary cross Entropy \n",
    "#cost = T.nnet.binary_crossentropy(noise_py_x, Y).mean()\n",
    "\n",
    "#updates = RMSprop(cost, params, lr=0.005)\n",
    "grads = T.grad(cost, params)\n",
    "updates = [ (param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(params, grads)]\n",
    "    \n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#Predict Loop\n",
    "l1, l2, l3, l4, l5,l6, py_x = model(X, w, b, w2, b2, w3, b3, w4, b4, w5, b5, w6, b6, w_o, b_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Error: 0.300134138162 Cost: 1.66613554955\n",
      "Epoch: 1  Error: 0.300134138162 Cost: 1.64913547039\n",
      "Epoch: 2  Error: 0.300134138162 Cost: 1.65275764465\n",
      "Epoch: 3  Error: 0.300134138162 Cost: 1.65269625187\n",
      "Epoch: 4  Error: 0.300134138162 Cost: 1.6541634798\n",
      "Epoch: 5  Error: 0.311871227364 Cost: 1.6295375824\n",
      "Epoch: 6  Error: 0.391683433937 Cost: 1.60018861294\n",
      "Epoch: 7  Error: 0.389336016097 Cost: 1.5419921875\n",
      "Epoch: 8  Error: 0.395707578806 Cost: 1.52473008633\n",
      "Epoch: 9  Error: 0.404761904762 Cost: 1.5251185894\n",
      "Epoch: 10  Error: 0.417840375587 Cost: 1.47361445427"
     ]
    }
   ],
   "source": [
    "oldError = 0.\n",
    "startError = 0.\n",
    "iterStart = 0\n",
    "iterEnd = 2001\n",
    "\n",
    "for i in range(iterStart,iterEnd):\n",
    "    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    error = np.mean(np.argmax(teY, axis=1) == predict(teX))\n",
    "    \n",
    "    if(i == iterStart):\n",
    "        oldError = error\n",
    "        startError = error\n",
    "    \n",
    "    delta = ((error - oldError)/oldError)*100\n",
    "    improvement = ((startError - error)/startError)*100\n",
    "    \n",
    "    if (delta > 0.2 or (i%100 == 0)):\n",
    "        saveModel(i)\n",
    "    if (improvement > 1.0):\n",
    "        saveModel(i)\n",
    "        startError = error\n",
    "            \n",
    "    logline = \"Epoch: \" + str(i) + \"  Error: \" + str(error) + \" Cost: \" + str(cost)\n",
    "    print logline\n",
    "    f = open(logPickle, 'a+')\n",
    "    cPickle.dump(logline , f);\n",
    "    f.close()\n",
    "    oldError = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = predict(teX)\n",
    "print predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real = np.argmax(teY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion = np.zeros((6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0 , len(real)):\n",
    "    j = real[i]\n",
    "    k = predicted[i]\n",
    "    #print \"Real:  %d , predicted %d\"%(j,k)\n",
    "    confusion[j][k] = confusion[j][k] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(0,7):\n",
    "    #print (confusion[j][j]/sum(confusion[j][:]))\n",
    "    print (sum(confusion[j][:]) - confusion[j][j])/sum(confusion[j][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.histogram(real , bins=[0,1, 2, 3 , 4 , 5 , 6 , 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
