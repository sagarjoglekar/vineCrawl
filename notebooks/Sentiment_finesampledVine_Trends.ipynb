{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from dataUtils import *\n",
    "import cPickle as pickle\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from scipy.interpolate import UnivariateSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ClassFile = \"../Logs/classes.json\"\n",
    "ClassFile = \"../Logs/english_label.txt\"\n",
    "\n",
    "#vineScores = \"../Logs/sampled_vine_sentibank_final.csv\"\n",
    "vineScores = \"../Logs/MVSO_fine_vine_probs.csv\"\n",
    "\n",
    "#selfieScores = \"../Logs/selfieSentibankProbs.csv\"\n",
    "sentibank_scores = \"../Logs/sentibank_baseline_final.csv\"\n",
    "\n",
    "#vineANPs = \"../Logs/sampled_vine_ANPS_final.pk\"\n",
    "vineANPs = \"../Logs/MVSO_fine_vine_ANPs.pk\"\n",
    "#selfiePaths = \"../Logs/selfiePaths.txt\"\n",
    "\n",
    "imageNetObjs = \"../Logs/sampledvineImagenetObjs2015_1.pk\"\n",
    "\n",
    "#selfiePopularityFile = \"../Logs/selfie_dataset.txt\"\n",
    "\n",
    "root = \"../vinedata/Data/\"\n",
    "\n",
    "visitedList = \"../Logs/sampledVids.data\"\n",
    "\n",
    "postFile = \"../Logs/postsMapping.csv\"\n",
    "\n",
    "# sentimentFile = \"../Logs/ANP_Sentiments.txt\"\n",
    "\n",
    "color_features = \"../Logs/vine_features_ordered.csv\"\n",
    "\n",
    "sampled_img_list = \"../Logs/sampled_sentibank_image.txt\"\n",
    "\n",
    "revisedSentimentFile = \"../Logs/revised_ANP_sentiments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the Sentibank scores for Vines\n",
    "vineProbs = np.loadtxt(vineScores, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the Sentibank scores for Baseline\n",
    "sentibank_baseline = []\n",
    "#sentibank_baseline = np.loadtxt(sentibank_scores, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vineProbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ANP_ID_List(pickleList , nameIdx):\n",
    "    ANPs = []\n",
    "    IDs = []\n",
    "    for line in pickleList:\n",
    "        ids = line.split(',')[0].split('/')[nameIdx].split('_')[0]\n",
    "        IDs.append(ids)\n",
    "        ANPs.append(line.split(',')[1])\n",
    "    return IDs , ANPs\n",
    "\n",
    "def get_vid_senti(pickle , index , nameidx):\n",
    "    oldId = pickle[index].split(',')[0].split('/')[nameidx].split('_')[0]\n",
    "    seqDict = dict()\n",
    "    indexList = []\n",
    "    sequence = pickle[index].split(',')[0].split('/')[nameidx].split('_')[1].split('.')[0]\n",
    "    seqDict[int(sequence)] = str(pickle[index].split(',')[1])\n",
    "    indexList.append(index)\n",
    "    index+=1\n",
    "    #print index\n",
    "    while (index < len(pickle) and (pickle[index].split(',')[0].split('/')[nameidx].split('_')[0] == oldId)):\n",
    "        sequence = pickle[index].split(',')[0].split('/')[nameidx].split('_')[1].split('.')[0]\n",
    "        seqDict[int(sequence)] = str(pickle[index].split(',')[1])\n",
    "        indexList.append(index)\n",
    "        index += 1\n",
    "    seqDict['indexList'] = indexList\n",
    "    return seqDict , oldId , index\n",
    "\n",
    "#This function maps each video with a dictionary entry that has list of all ANPS per frame\n",
    "#and an index list to find them\n",
    "def get_VID_ANP_List(pickle, nameidx):\n",
    "    megaDict = dict()\n",
    "    i = 0\n",
    "    print len(pickle)\n",
    "    while i < len(pickle):           \n",
    "        subDict , postId , i = get_vid_senti(pickle , i , nameidx)\n",
    "        megaDict[int(postId)] = subDict\n",
    "    return megaDict\n",
    "\n",
    "\n",
    "def pruneMegaDict(megadict , filterindices):\n",
    "    filteredList = dict()\n",
    "    for entry in megadict:\n",
    "        commns = set(megadict[entry]['indexList']).intersection(filterindices)\n",
    "        if len(commns) >= 6:\n",
    "            filteredList[entry] = megadict[entry]\n",
    "    return filteredList\n",
    "\n",
    "def readJson(path):\n",
    "    f = open(path)\n",
    "    data = json.loads(f.read())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_list =[]\n",
    "with open(sampled_img_list) as f:\n",
    "    image_list = f.readlines()\n",
    "\n",
    "true_labels = []\n",
    "for line in image_list:\n",
    "    label = line.split('/')[5]\n",
    "    true_labels.append(label)\n",
    "    \n",
    "# f = open(ClassFile ,'r')\n",
    "# sentibankClasses = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "f = open(ClassFile ,'r')\n",
    "sentibankClasses = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentibank_probs = np.zeros(len(sentibank_baseline))\n",
    "# detected_labels = []\n",
    "# for i in range(sentibank_probs.shape[0]):\n",
    "#     sentibank_probs[i] = sentibank_baseline[i].max()\n",
    "#     detected_labels.append(str(sentibankClasses[np.argmax(sentibank_baseline[i])]))\n",
    "    \n",
    "# print np.median(sentibank_probs), sentibank_probs.var(), sentibank_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxVecs = np.zeros(len(vineProbs))\n",
    "for i in range(len(vineProbs)):\n",
    "    maxVecs[i] = np.max(vineProbs[i])\n",
    "print np.mean(maxVecs) , np.median(maxVecs) , np.var(maxVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curatedProbs_index = []\n",
    "for i in range(len(vineProbs)):\n",
    "    if (vineProbs[i].max() > (np.mean(maxVecs))):\n",
    "        curatedProbs_index.append(i)\n",
    "\n",
    "        \n",
    "print len(curatedProbs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vineList = readLists(vineANPs)\n",
    "objList = readLists(imageNetObjs)\n",
    "idList, anpList = get_ANP_ID_List(vineList , 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print anpList[5]\n",
    "print idList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vineList[0].split(',')[0].split('/')[6].split('_')[0]\n",
    "print vineList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentidict0 = readSentiments()\n",
    "sentidict = readRevisedSentiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiArray = []\n",
    "for k in sentidict:\n",
    "    sentiArray.append(sentidict[k])\n",
    "print len(sentiArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.hist(sentiArray, 1000, normed=1,histtype='step', cumulative=True,linewidth = 3.0)\n",
    "plt.title(\"Distribution of ANP sentiments\", fontsize = 20)\n",
    "plt.xlabel(\"Sentiment value\",fontsize = 15)\n",
    "plt.ylabel(\"Frequency\",fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = open('sentibankANP.csv', 'wb')\n",
    "# for k,v in sentidict0.iteritems():\n",
    "#     f.write(k+','+str(v)+'\\n')\n",
    "#     #print k , v\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "megaDict = get_VID_ANP_List(vineList , 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print megaDict[1281867050901532672]\n",
    "# videoSentimentIndices = megaDict[1281867050901532672]['indexList']\n",
    "# for i in videoSentimentIndices:\n",
    "#     print vineList[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filteredDict = pruneMegaDict(megaDict, curatedProbs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(filteredDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(megaDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postIds = []\n",
    "for line in vineList:\n",
    "    arr = line.split('/')\n",
    "    i = int(arr[6].split('_')[0])\n",
    "    postIds.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectors for filtered videos\n",
    "senti_matrix = np.zeros((len(filteredDict),12))\n",
    "ANP_matrix = list()\n",
    "print senti_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "misses = 0\n",
    "postIdFilteredList = []\n",
    "for entry in filteredDict:\n",
    "    postIdFilteredList.append(entry)\n",
    "    ANPEntry = list()\n",
    "    for j in range(1,13):\n",
    "        \n",
    "        if j in filteredDict[entry]:\n",
    "            senti_matrix[i][j-1] = sentidict[filteredDict[entry][j]] \\\n",
    "            if (filteredDict[entry][j] in sentidict) \\\n",
    "            else sentidict[filteredDict[entry][j-1]]\n",
    "            \n",
    "            ANPEntry.append(filteredDict[entry][j])\n",
    "        else:\n",
    "            last = ANPEntry[-1]\n",
    "            ANPEntry.append(last)\n",
    "            senti_matrix[i][j-1] = senti_matrix[i][j-2]\n",
    "            misses += 1\n",
    "    ANP_matrix.append(ANPEntry)\n",
    "    i += 1\n",
    "print misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersect(a, b):\n",
    "    result=[]\n",
    "    for i in b:\n",
    "        if isinstance(i,list):\n",
    "            result.append(intersect(a,i))\n",
    "        else:\n",
    "            if i in a:\n",
    "                 result.append(i)\n",
    "    return result\n",
    "\n",
    "def JaccardSim(l1 , l2):\n",
    "    intersection = list(set(l1).intersection(l2))\n",
    "    union = list(set(l1).union(l2))\n",
    "    #intersection = intersect(l1 , l2)\n",
    "    jaccard = float(len(intersection)/len(union))*1.0\n",
    "    #jaccard = float(len(intersection)/(len(l1) + len(l2)))*1.0\n",
    "    return jaccard\n",
    "\n",
    "def similarityMatrix(ANPList):\n",
    "    simMatrix = np.zeros((len(ANPList),len(ANPList)))\n",
    "    for i in range(len(ANPList)):\n",
    "        for j in range(len(ANPList)):\n",
    "            jacSim = JaccardSim(ANPList[i],ANPList[j])\n",
    "            simMatrix[i][j] = jacSim\n",
    "    return simMatrix\n",
    "\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[result.size/2:]\n",
    "\n",
    "def findAutocorr(sentimatrix):\n",
    "    autocorr_matrix = []\n",
    "    for i in range(senti_matrix.shape[0]):\n",
    "        autocorr_matrix.append(list(autocorr(senti_matrix[i])))\n",
    "    return np.asarray(autocorr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled = np.random.choice(len(senti_matrix) , 2999)\n",
    "sampled_matrix = senti_matrix[sampled,:]\n",
    "print sampled_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autocorr_matrix = findAutocorr(sampled_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print autocorr_matrix.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Jaccard_Matrix = similarityMatrix(ANP_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatMap = Jaccard_Matrix\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(30, 30)\n",
    "plt.imshow(heatMap[1000:2000,1000:2000], cmap='hot' , interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get all post meta data \n",
    "with open(postFile, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "postDict = {}\n",
    "postList = []\n",
    "for line in lines:\n",
    "    comp = line.split(',')\n",
    "    postDict[comp[0]] = comp[1]\n",
    "    postList.append(comp[1])\n",
    "\n",
    "print len(postDict) , postList[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selectedPosts = []\n",
    "allPosts = []\n",
    "\n",
    "for d in postList:\n",
    "    dataRoot = \"/datasets/sagarj/vineData/Dataset/Posts/\" + d.strip()\n",
    "    f = open(dataRoot , 'rb')\n",
    "    data = json.loads(f.read())\n",
    "    allPosts.append(data['data']['records'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(allPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredPosts = dict()\n",
    "for k in filteredDict:\n",
    "    for post in allPosts:\n",
    "        if post['postId'] == k:\n",
    "            filteredPosts[k] = post\n",
    "print len(filteredPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "rand_smpl = [ allPosts[i] for i in sorted(random.sample(xrange(len(allPosts)), len(filteredPosts))) ]\n",
    "print len(rand_smpl)\n",
    "\n",
    "allLikes = []\n",
    "allReposts = []\n",
    "allLoops = []\n",
    "for rec in rand_smpl:\n",
    "    allLikes.append(rec['likes']['count'])\n",
    "    allReposts.append(rec['reposts']['count'])\n",
    "    allLoops.append(rec['loops']['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(allLikes) , np.max(allLikes) , np.min(allReposts) , np.max(allReposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filLikes = []\n",
    "filReposts = []\n",
    "filLoops = []\n",
    "filPosts = []\n",
    "for k in filteredPosts:\n",
    "    filLikes.append(np.log(filteredPosts[k]['likes']['count']+1))\n",
    "    filReposts.append(np.log(filteredPosts[k]['reposts']['count']+1))\n",
    "    filLoops.append(np.log(filteredPosts[k]['loops']['count']+1))\n",
    "    filPosts.append(filteredPosts[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.hist(filLikes, 400, normed=1,histtype='step', cumulative=True,linewidth = 3.0,linestyle='dashed')\n",
    "plt.hist(filReposts, 400, normed=1,histtype='step', cumulative=True,linewidth = 3.0)\n",
    "plt.title(\"CDF for Likes and Repost\", fontsize = 25)\n",
    "plt.xlabel(\"Likes/repost count (Log scale)\",fontsize = 25)\n",
    "plt.ylabel(\"CDF\",fontsize = 25)\n",
    "plt.legend(['Likes' , 'Reposts'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortedWRTLikesPosts = [x for (y,x) in sorted(zip(filLikes,filPosts))]\n",
    "sortedWRTRepostsPosts = [x for (y,x) in sorted(zip(filReposts,filPosts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(filLikes) , np.max(filLikes) , np.min(filReposts) , np.max(filReposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.median(filLikes)\n",
    "print np.median(filReposts)\n",
    "print np.median(np.log(allLikes))\n",
    "print np.median(np.log(allReposts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.corrcoef(filReposts,filLoops)\n",
    "print np.corrcoef(filLikes,filLoops)\n",
    "print np.corrcoef(filReposts,filLikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simlist = []\n",
    "simcount = []\n",
    "dummy_Jaccard = Jaccard_Matrix.copy()\n",
    "for i in range(dummy_Jaccard.shape[0]):\n",
    "    sim = []\n",
    "    count = 0\n",
    "    for j in range(dummy_Jaccard.shape[1]):\n",
    "        if dummy_Jaccard[i][j] == 1.0:\n",
    "            sim.append(j)\n",
    "            dummy_Jaccard[j][i] = 0.0\n",
    "            count += 1\n",
    "    simlist.append(sim)\n",
    "    simcount.append(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxMatches = np.sort(simcount)[3000:3100]\n",
    "matchIndexes = np.argsort(simcount)[3000:3100]\n",
    "\n",
    "print maxMatches , matchIndexes\n",
    "print np.argsort(simcount)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15, 10)\n",
    "# plt.plot(simcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def dotproduct(v1, v2):\n",
    "    return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "def length(v):\n",
    "    return math.sqrt(dotproduct(v, v))\n",
    "\n",
    "def angle(v1, v2):\n",
    "    return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "def topNColliniear(basis , vectors , topN):\n",
    "    ang = []\n",
    "    for vec in vectors:\n",
    "        ang.append(angle_between(vec , basis))\n",
    "    indexes = np.argsort(ang)\n",
    "    print ang[indexes[0]],ang[indexes[-1]]\n",
    "    print indexes[:topN]\n",
    "    #returns the top N indexes with least angular value\n",
    "    return indexes[:topN] , vectors[indexes[:topN]]\n",
    "\n",
    "def getANPs(postDict , posts):\n",
    "    ANP_list = []\n",
    "    ids = [i['postId'] for i in posts]\n",
    "    for postid in ids:\n",
    "        ANP_list.append(postDict[postid])\n",
    "    return ANP_list\n",
    "\n",
    "def filterCluster(postids , posts , postDict, indexList):\n",
    "    plist = []\n",
    "    ANP_list = []\n",
    "    ids = [postids[i] for i in indexList ]\n",
    "    for postid in ids:\n",
    "        plist.append(posts[postid])\n",
    "        ANP_list.append(postDict[postid])\n",
    "    return plist , ANP_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vidnumber = matchIndexes[35]\n",
    "print np.sum(dummy_Jaccard[vidnumber,:])\n",
    "matched = []\n",
    "for i in range(len(dummy_Jaccard[vidnumber])):\n",
    "    if dummy_Jaccard[vidnumber][i]== 1.0:\n",
    "        matched.append(i)\n",
    "print matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# index1 , topSentis1 = topNColliniear(principals[0], senti_matrix , 3) \n",
    "matchedPosts , ANPmatched  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,matched)\n",
    "#print ANPmatched\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "num = 2\n",
    "url = matchedPosts[num]['videoUrl'].strip().split('/')[-1].split('?')[0]\n",
    "print  matchedPosts[num]['videoUrl']\n",
    "#print videosBasis1[num]\n",
    "print url\n",
    "videoPath = \"/datasets/sagarj/vineData/Dataset/Videos/\" + url\n",
    "#print videoPath\n",
    "\n",
    "\n",
    "video = io.open(videoPath, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "clf = LinearSVR()\n",
    "\n",
    "clf.fit(senti_matrix, filReposts) \n",
    "print clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print senti_matrix[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_vector = np.reshape(senti_matrix, senti_matrix.shape[0]*senti_matrix.shape[1])\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.hist(senti_vector, 20, normed=1,histtype='step', cumulative=False,linewidth = 3.0)\n",
    "plt.title(\"Distribution of all frames\", fontsize = 20)\n",
    "plt.xlabel(\"Sentiment value\",fontsize = 15)\n",
    "plt.ylabel(\"CDF\",fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxGraph = np.zeros(senti_matrix.shape[1])\n",
    "minGraph = np.zeros(senti_matrix.shape[1])\n",
    "varMaxGraph = np.zeros(senti_matrix.shape[1])\n",
    "varMinGraph = np.zeros(senti_matrix.shape[1])\n",
    "differenceList = []\n",
    "id1 = []\n",
    "id2 = []\n",
    "for i in range(len(senti_matrix)):\n",
    "    a = max(senti_matrix[i])\n",
    "    b = min(senti_matrix[i])\n",
    "    median_sent = np.median(senti_matrix[i])\n",
    "    diff = a - b\n",
    "    \n",
    "    diff2 = a - median_sent\n",
    "    if diff > 0.5:\n",
    "        ind1 = senti_matrix[i].tolist().index(a)\n",
    "        if ind1 < 2:\n",
    "             id1.append(i)\n",
    "        maxGraph[ind1] += 1\n",
    "        ind2 = senti_matrix[i].tolist().index(b)\n",
    "        if ind2 < 2:\n",
    "             id2.append(i)\n",
    "        minGraph[ind2] += 1\n",
    "        differenceList.append(diff)\n",
    "    \n",
    "    \n",
    "maxGraph = maxGraph/senti_matrix.shape[0]\n",
    "minGraph = minGraph/senti_matrix.shape[0]\n",
    "print maxGraph\n",
    "print minGraph\n",
    "print senti_matrix.shape\n",
    "print np.sum(maxGraph),np.sum(minGraph)\n",
    "print len(id1) , len(id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c3 = list(set(id1).intersection(id2))\n",
    "len(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "n, bins, patches = plt.hist(differenceList, 10, normed=0,histtype='step', cumulative=False,linewidth = 3.0)\n",
    "plt.title(\"Distribution of difference between maximum and minimum sentiment in a video\", fontsize = 20)\n",
    "plt.xlabel(\"Sentiment value\",fontsize = 15)\n",
    "plt.ylabel(\"CDF\",fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.title(\"Frequency of finding max/min sentiment for a given video in nth 0.5 second interval (dropping constant sentiment videos)\", fontsize = 15)\n",
    "plt.xlabel(\"Nth 0.5 second interval into the video\",fontsize = 15)\n",
    "plt.ylabel(\"Frequency\",fontsize = 15)\n",
    "plt.plot(maxGraph,linewidth = 3.0)\n",
    "plt.plot(minGraph,linewidth = 3.0)\n",
    "plt.legend(['Max Value', 'Min Value'   \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq\n",
    "from scipy.spatial.distance import minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroid_list = []\n",
    "id_list = []\n",
    "for i in range(1 , 12):\n",
    "    centroids,_ = kmeans(senti_matrix,i)\n",
    "    idx,_ = vq(senti_matrix,centroids)\n",
    "    centroid_list.append(centroids)\n",
    "    id_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSE_values = np.zeros(len(id_list))\n",
    "for i in range(len(centroid_list)):\n",
    "    for j in range(len(centroid_list[i])):\n",
    "        vecs = senti_matrix[id_list[i]==j,:]\n",
    "        #print vecs.shape\n",
    "        cent = centroid_list[i][j]\n",
    "        SSE_1 = 0.0\n",
    "        for vec in vecs:\n",
    "            SSE_1 = SSE_1 + minkowski(vec,cent,2)\n",
    "        SSE_values[j] = SSE_values[j] + SSE_1\n",
    "for i in range(len(SSE_values)):\n",
    "    SSE_values[i] = SSE_values[i]/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "x = np.arange(1,11)\n",
    "plt.plot(x,SSE_values[:10] ,linewidth = 3.0)\n",
    "plt.xlabel(\"Number of Clusters\", fontsize = 25)\n",
    "plt.ylabel(\"Mean Minkowski distance from cluster centroids\", fontsize = 25)\n",
    "plt.title(\"Cluster groupings for Sentiment vectors for Videos\", fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print SSE_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(senti_matrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print U.shape,  np.diag(s).shape, V.shape\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "low_dim_senti_matrix = pca.fit_transform(senti_matrix)\n",
    "print low_dim_senti_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variances = pca.explained_variance_ratio_\n",
    "print variances[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "principals=  2.5 + 5*pca.components_\n",
    "#principals=  pca.components_\n",
    "print principals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying to cluster in the SVD space \n",
    "\n",
    "low_dim_centroid_list = []\n",
    "low_dim_id_list = []\n",
    "for i in range(1 , 12):\n",
    "    centroids,_ = kmeans(low_dim_senti_matrix,i)\n",
    "    idx,_ = vq(low_dim_senti_matrix,centroids)\n",
    "    low_dim_centroid_list.append(centroids)\n",
    "    low_dim_id_list.append(idx)\n",
    "    \n",
    "low_dim_SSE_values = np.zeros(len(low_dim_id_list))\n",
    "for i in range(len(low_dim_centroid_list)):\n",
    "    for j in range(len(low_dim_centroid_list[i])):\n",
    "        vecs = low_dim_senti_matrix[id_list[i]==j,:]\n",
    "        #print vecs.shape\n",
    "        cent = low_dim_centroid_list[i][j]\n",
    "        SSE_1 = 0.0\n",
    "        for vec in vecs:\n",
    "            SSE_1 = SSE_1 + minkowski(vec,cent,2)\n",
    "        low_dim_SSE_values[j] = low_dim_SSE_values[j] + SSE_1\n",
    "for i in range(len(low_dim_SSE_values)):\n",
    "    low_dim_SSE_values[i] = low_dim_SSE_values[i]/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "x = np.arange(1,11)\n",
    "plt.plot(x,low_dim_SSE_values[:10] ,linewidth = 3.0)\n",
    "plt.xlabel(\"Number of Clusters\", fontsize = 25)\n",
    "plt.ylabel(\"Mean Minkowski distance from cluster centroids\", fontsize = 25)\n",
    "plt.title(\"Cluster groupings for Sentiment vectors for Videos\", fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_dim_centroid_final,_ = kmeans(low_dim_senti_matrix,3)\n",
    "low_dim_idx_final,_ = vq(low_dim_senti_matrix,low_dim_centroid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Sentiment transition graphs for all cluster centroids in Latent PC space\", fontsize = 25)\n",
    "plt.plot(low_dim_centroid_final[0], linewidth = 2.0)\n",
    "plt.plot(low_dim_centroid_final[1], linewidth = 2.0)\n",
    "plt.plot(low_dim_centroid_final[2], linewidth = 2.0)\n",
    "plt.plot(low_dim_centroid_final[3], linewidth = 2.0)\n",
    "#plt.plot(centroid_final[4], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[5], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[6], linewidth = 2.0 )\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Cluster 1', 'Cluster 2' , 'Cluster 3' \n",
    "            ,'Cluster 4'\n",
    "            ,'Cluster 5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# S = np.diag(s)\n",
    "# Mhat = np.dot(U, np.dot(S, V.T))\n",
    "# print Mhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print \"Using all PCs, MSE = %.6G\" %(np.mean((senti_matrix - Mhat)**2)/len(senti_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mhat2 = np.dot(U[:, :1], np.dot(S[:1, :1], V[:,:1].T))\n",
    "# print Mhat2.shape\n",
    "# print \"Using first 20 PCs, MSE = %.6G\" %(np.mean((senti_matrix - Mhat2)**2)/len(senti_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# norms = np.linalg.norm(Modes,axis=1)\n",
    "\n",
    "# indexlist = np.argsort(np.linalg.norm(Modes,axis=1))\n",
    "# sorted_modes = Modes[indexlist]\n",
    "# norms = np.linalg.norm(sorted_modes,axis=1)\n",
    "# plt.plot(s)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print sorted_modes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Plot of top 5 left Singular Vectors\", fontsize = 25)\n",
    "plt.plot(principals[0], linewidth = 2.0)\n",
    "plt.plot(principals[1], linewidth = 2.0)\n",
    "plt.plot(principals[2], linewidth = 2.0)\n",
    "plt.plot(principals[3], linewidth = 2.0)\n",
    "plt.plot(principals[4], linewidth = 2.0 )\n",
    "#plt.plot(principals[5], linewidth = 2.0 )\n",
    "#plt.plot(principals[6], linewidth = 2.0 )\n",
    "#plt.plot(sorted_modes[7,1:], linewidth = 2.0 )\n",
    "#plt.plot(principals[8,1:], linewidth = 2.0 )\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Cluster 1', 'Cluster 2' , 'Cluster 3' \n",
    "            ,'Cluster 4'\n",
    "            ,'Cluster 5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index1 , topSentis1 = topNColliniear(principals[0], senti_matrix , 3) \n",
    "videosBasis1 , ANP1  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index1)\n",
    "\n",
    "index2 , topSentis2 = topNColliniear(principals[1], senti_matrix , 3) \n",
    "videosBasis2 , ANP2  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index2)\n",
    "\n",
    "index3 , topSentis3 = topNColliniear(principals[2], senti_matrix , 3) \n",
    "videosBasis3 , ANP3  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index3)\n",
    "\n",
    "index4 , topSentis4 = topNColliniear(principals[3], senti_matrix , 3) \n",
    "videosBasis4 , ANP4  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index4)\n",
    "\n",
    "index5 , topSentis5 = topNColliniear(principals[4], senti_matrix , 3) \n",
    "videosBasis5 , ANP5  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ANP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectedid =  videosBasis5[2]['postId']\n",
    "print megaDict[selectedid]\n",
    "videoSentimentIndices = megaDict[selectedid]['indexList']\n",
    "for i in range(len(videoSentimentIndices)):\n",
    "    a = vineList[videoSentimentIndices[i]].split(',')\n",
    "    print \"Image %d :  %s\" %(i , str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k in ANP1:\n",
    "#     indexes = k['indexList']\n",
    "#     for i in indexes:\n",
    "#         print vineList[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (180/3.142)*0.034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "num = 2\n",
    "url = videosBasis5[num]['videoUrl'].strip().split('/')[-1].split('?')[0]\n",
    "print  videosBasis5[num]['videoUrl']\n",
    "#print videosBasis1[num]\n",
    "#print url\n",
    "videoPath = \"/datasets/sagarj/vineData/Dataset/Videos/\" + url\n",
    "#print videoPath\n",
    "\n",
    "\n",
    "video = io.open(videoPath, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Sentiment Plot of top collinear sentiment vectors for all basis\", fontsize = 25)\n",
    "plt.plot(topSentis1[0], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis2[0], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis3[0], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis4[0], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis5[0], linewidth = 2.0 , linestyle='dashed')\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Basis 1', 'Basis 2' , 'Basis 3' \n",
    "            ,'Basis 4'\n",
    "            ,'Basis 5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroid_final,_ = kmeans(senti_matrix,4)\n",
    "idx_final,_ = vq(senti_matrix,centroid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs1 = senti_matrix[idx_final==0,:]\n",
    "vecs2 = senti_matrix[idx_final==1,:]\n",
    "vecs3 = senti_matrix[idx_final==2,:]\n",
    "vecs4 = senti_matrix[idx_final==3,:]\n",
    "# vecs5 = senti_matrix[idx_final==4,:]\n",
    "# vecs6 = senti_matrix[idx_final==5,:]\n",
    "# vecs7 = senti_matrix[idx_final==6,:]\n",
    "#senti_matrix[idx_final==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(vecs1)\n",
    "print len(vecs2)\n",
    "print len(vecs3)\n",
    "print len(vecs4)\n",
    "# print len(vecs5)\n",
    "# print len(vecs6)\n",
    "# print len(vecs7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print centroid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Sentiment transition graphs for all cluster centroids\", fontsize = 25)\n",
    "plt.plot(centroid_final[0], linewidth = 2.0)\n",
    "plt.plot(centroid_final[1], linewidth = 2.0)\n",
    "plt.plot(centroid_final[2], linewidth = 2.0)\n",
    "plt.plot(centroid_final[3], linewidth = 2.0)\n",
    "#plt.plot(centroid_final[4], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[5], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[6], linewidth = 2.0 )\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Cluster 1', 'Cluster 2' , 'Cluster 3' \n",
    "            ,'Cluster 4'\n",
    "            ,'Cluster 5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index1_clusters , topSentis1_clusters = topNColliniear(centroid_final[0], senti_matrix[idx_final==0,:] , 100) \n",
    "videosBasis1_clusters , ANP1_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index1_clusters)\n",
    "\n",
    "index2_clusters , topSentis2_clusters = topNColliniear(centroid_final[1], senti_matrix[idx_final==1,:] , 100) \n",
    "videosBasis2_clusters , ANP2_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index2_clusters)\n",
    "\n",
    "index3_clusters , topSentis3_clusters = topNColliniear(centroid_final[2], senti_matrix[idx_final==2,:] , 100) \n",
    "videosBasis3_clusters , ANP3_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index3_clusters)\n",
    "\n",
    "index4_clusters , topSentis4_clusters = topNColliniear(centroid_final[3], senti_matrix[idx_final==3,:] , 100) \n",
    "videosBasis4_clusters , ANP4_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index4_clusters)\n",
    "\n",
    "# index5_clusters , topSentis5_clusters = topNColliniear(centroid_final[4], senti_matrix , 5) \n",
    "# videosBasis5_clusters , ANP5_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index5_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Sentiment Plot of top collinear sentiment vectors for all Cluster centroids\", fontsize = 25)\n",
    "plt.plot(topSentis1_clusters[50], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis2_clusters[50], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis3_clusters[50], linewidth = 2.0 , linestyle='dashed')\n",
    "plt.plot(topSentis4_clusters[50], linewidth = 2.0 , linestyle='dashed')\n",
    "#plt.plot(topSentis5_clusters[4], linewidth = 2.0 , linestyle='dashed')\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Centroid for C1', 'Centroid for C2' , 'Centroid for C3' \n",
    "            ,'Centroid for C4'\n",
    "            ,'Centroid for C5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selectedid =  videosBasis4_clusters[2]['postId']\n",
    "print megaDict[selectedid]\n",
    "videoSentimentIndices = megaDict[selectedid]['indexList']\n",
    "for i in range(len(videoSentimentIndices)):\n",
    "    a = vineList[videoSentimentIndices[i]].split(',')\n",
    "    print \"Image %d :  %s\" %(i , str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "num = 2\n",
    "url = videosBasis4_clusters[num]['videoUrl'].strip().split('/')[-1].split('?')[0]\n",
    "print  videosBasis4_clusters[num]['videoUrl']\n",
    "print  videosBasis4_clusters[num]['postId']\n",
    "#print videosBasis1[num]\n",
    "#print url\n",
    "videoPath = \"/datasets/sagarj/vineData/Dataset/Videos/\" + url\n",
    "#print videoPath\n",
    "\n",
    "\n",
    "video = io.open(videoPath, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index1_clusters , topSentis1_clusters = topNColliniear(centroid_final[0], senti_matrix[idx_final==0,:] , 100) \n",
    "videos1_clusters , ANP1_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index1_clusters)\n",
    "\n",
    "index2_clusters , topSentis2_clusters = topNColliniear(centroid_final[1], senti_matrix[idx_final==1,:] , 100) \n",
    "videos2_clusters , ANP2_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index2_clusters)\n",
    "\n",
    "index3_clusters , topSentis3_clusters = topNColliniear(centroid_final[2], senti_matrix[idx_final==2,:] , 100) \n",
    "videos3_clusters , ANP3_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index3_clusters)\n",
    "\n",
    "index4_clusters , topSentis4_clusters = topNColliniear(centroid_final[3], senti_matrix[idx_final==3,:] , 100) \n",
    "videos4_clusters , ANP4_clusters  = filterCluster(postIdFilteredList,filteredPosts,filteredDict,index4_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectedid =  videos1_clusters[2]['postId']\n",
    "print megaDict[selectedid]\n",
    "videoSentimentIndices = megaDict[selectedid]['indexList']\n",
    "for i in range(len(videoSentimentIndices)):\n",
    "    a = vineList[videoSentimentIndices[i]].split(',')\n",
    "    print \"Image %d :  %s\" %(i , str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filterCluster(postids , posts , postDict, clusterMems , clusterId ):\n",
    "    plist = []\n",
    "    ANP_list = []\n",
    "    selective = clusterMems == clusterId\n",
    "    ids = [i for (i, v) in zip(postids, selective) if v]\n",
    "    for postid in ids:\n",
    "        plist.append(posts[postid])\n",
    "        ANP_list.append(postDict[postid])\n",
    "    return plist , ANP_list\n",
    "\n",
    "def likesRepostsLoops(cluster):\n",
    "    likes = np.zeros(len(cluster))\n",
    "    reposts = np.zeros(len(cluster))\n",
    "    loops = np.zeros(len(cluster))\n",
    "    for i in range(len(cluster)):\n",
    "        likes[i] = np.log(cluster[i]['likes']['count']+1)\n",
    "        reposts[i] = np.log(cluster[i]['reposts']['count']+1)\n",
    "        loops[i] = np.log(cluster[i]['loops']['count']+1)\n",
    "    return likes, reposts, loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster1 , ANP_cluster1 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,0)\n",
    "print len(cluster1)\n",
    "cluster2 , ANP_cluster2 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,1)\n",
    "print len(cluster2)\n",
    "cluster3 , ANP_cluster3 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,2)\n",
    "print len(cluster3)\n",
    "cluster4 , ANP_cluster4 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,3)\n",
    "print len(cluster4)\n",
    "cluster5 , ANP_cluster5 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,4)\n",
    "print len(cluster5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likesC1 , repostsC1 , loopsC1 = likesRepostsLoops(cluster1)\n",
    "likesC2 , repostsC2 , loopsC2 = likesRepostsLoops(cluster2)\n",
    "likesC3 , repostsC3 , loopsC3 = likesRepostsLoops(cluster3)\n",
    "likesC4 , repostsC4 , loopsC4 = likesRepostsLoops(cluster4)\n",
    "likesC5 , repostsC5 , loopsC5 = likesRepostsLoops(cluster5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1 = np.random.choice(len(likesC1), 300)\n",
    "sampledLikesC1 = likesC1[c1]\n",
    "\n",
    "c2 = np.random.choice(len(likesC2), 300)\n",
    "sampledLikesC2 = likesC2[c2]\n",
    "\n",
    "c3 = np.random.choice(len(likesC3), 300)\n",
    "sampledLikesC3 = likesC3[c3]\n",
    "\n",
    "c4 = np.random.choice(len(likesC4), 300)\n",
    "sampledLikesC4 = likesC4[c4]\n",
    "\n",
    "# c5 = np.random.choice(len(likesC5), 300)\n",
    "# sampledLikesC5 = likesC5[c5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Numbers for Cluster C1: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC1),np.mean(repostsC1),np.median(sampledLikesC1),np.mean(sampledLikesC1),np.median(loopsC1),np.mean(loopsC1))\n",
    "\n",
    "print \"Numbers for Cluster C2: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC2),np.mean(repostsC2),np.median(sampledLikesC2),np.mean(sampledLikesC2),np.median(loopsC2),np.mean(loopsC2))\n",
    "\n",
    "print \"Numbers for Cluster C3: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC3),np.mean(repostsC3),np.median(sampledLikesC3),np.mean(sampledLikesC3),np.median(loopsC3),np.mean(loopsC3))\n",
    "\n",
    "print \"Numbers for Cluster C4: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC4),np.mean(repostsC4),np.median(sampledLikesC4),np.mean(sampledLikesC4),np.median(loopsC4),np.mean(loopsC4))\n",
    "\n",
    "# print \"Numbers for Cluster C5: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "# Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "# %f\"%(np.median(repostsC5),np.mean(repostsC5),np.median(sampledLikesC5),np.mean(sampledLikesC5),np.median(loopsC5),np.mean(loopsC5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVR()\n",
    "\n",
    "clf.fit(vecs3, repostsC3) \n",
    "print clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.hist(sampledLikesC1, 100, normed=1,histtype='step', cumulative=True,linewidth = 1.0)\n",
    "plt.hist(sampledLikesC2, 100, normed=1,histtype='step', cumulative=True,linewidth = 1.0)\n",
    "plt.hist(sampledLikesC3, 100, normed=1,histtype='step', cumulative=True, linewidth = 1.0)\n",
    "plt.hist(sampledLikesC4, 100, normed=1,histtype='step', cumulative=True, linewidth = 1.0)\n",
    "# plt.hist(sampledLikesC5, 100, normed=1,histtype='step', cumulative=True, linewidth = 1.0)\n",
    "plt.title(\"CDF for Likes for the 4 sentiment clusters \", fontsize = 25)\n",
    "plt.xlabel(\"Likes/repost count\",fontsize = 25)\n",
    "plt.ylabel(\"CDF\",fontsize = 25)\n",
    "plt.legend(['Likes C1', 'Likes C2' , 'Likes C3' , 'Likes C4' , 'Likes C5'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotDist(RV, samples, b):\n",
    "    N = samples \n",
    "    n = b\n",
    "    s = np.random.normal(size=N)   # generate your data sample with N elements\n",
    "    p1, x1 = np.histogram(RV, bins=n) # bin it into n = N/10 bins\n",
    "    xs1 = x1[:-1] + (x1[1] - x1[0])/2\n",
    "    f1 = UnivariateSpline(xs1, p1)\n",
    "    plt.plot(xs1 , f1(xs1) ,'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plotDist(sampledLikesC1 , 200 , 50)\n",
    "plotDist(sampledLikesC2 , 200 , 50)\n",
    "plotDist(sampledLikesC3 , 200 , 50)\n",
    "plotDist(sampledLikesC4 , 200 , 50)\n",
    "# plotDist(sampledLikesC5 , 200 , 50)\n",
    "plt.legend(['Reposts C1', 'Reposts C2' , 'Reposts C3' , 'Reposts C4', 'Reposts C4'])\n",
    "plt.xlabel(\"Like count per video per cluster in Log scale\",fontsize = 25)\n",
    "plt.ylabel(\"Frequency \",fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "num = 155\n",
    "url = cluster4[num]['videoDashUrl'].strip().split('/')[-1].split('?')[0]\n",
    "print  cluster4[num]['videoDashUrl']\n",
    "print ANP_cluster4[num]\n",
    "\n",
    "#print url\n",
    "videoPath = \"/datasets/sagarj/vineData/Dataset/Videos/\" + url\n",
    "print videoPath\n",
    "\n",
    "\n",
    "video = io.open(videoPath, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
