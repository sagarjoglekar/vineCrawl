{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from dataUtils import *\n",
    "import cPickle as pickle\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from scipy.interpolate import UnivariateSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ClassFile = \"../Logs/classes.json\"\n",
    "ClassFile = \"../Logs/english_label.txt\"\n",
    "\n",
    "#vineScores = \"../Logs/sampled_vine_sentibank_final.csv\"\n",
    "vineScores = \"../Logs/MVSO_fine_vine_probs.csv\"\n",
    "#selfieScores = \"../Logs/selfieSentibankProbs.csv\"\n",
    "sentibank_scores = \"../Logs/sentibank_baseline_final.csv\"\n",
    "\n",
    "#vineANPs = \"../Logs/sampled_vine_ANPS_final.pk\"\n",
    "vineANPs = \"../Logs/MVSO_fine_vine_ANPs.pk\"\n",
    "#selfiePaths = \"../Logs/selfiePaths.txt\"\n",
    "\n",
    "imageNetObjs = \"../Logs/sampledvineImagenetObjs2015_1.pk\"\n",
    "\n",
    "#selfiePopularityFile = \"../Logs/selfie_dataset.txt\"\n",
    "\n",
    "root = \"../vinedata/Data/\"\n",
    "\n",
    "visitedList = \"../Logs/sampledVids.data\"\n",
    "\n",
    "# sentimentFile = \"../Logs/ANP_Sentiments.txt\"\n",
    "\n",
    "color_features = \"../Logs/vine_features_ordered.csv\"\n",
    "\n",
    "sampled_img_list = \"../Logs/sampled_sentibank_image.txt\"\n",
    "\n",
    "revisedSentimentFile = \"../Logs/revised_ANP_sentiments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the Sentibank scores for both selfies and Vines\n",
    "vineProbs = np.loadtxt(vineScores, delimiter=',')\n",
    "sentibank_baseline = np.loadtxt(sentibank_scores, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vineProbs.shape , sentibank_baseline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ANP_ID_List(pickleList):\n",
    "    ANPs = []\n",
    "    IDs = []\n",
    "    for line in pickleList:\n",
    "        ids = line.split(',')[0].split('/')[5].split('_')[0]\n",
    "        IDs.append(ids)\n",
    "        ANPs.append(line.split(',')[1])\n",
    "    return IDs , ANPs\n",
    "\n",
    "def get_vid_senti(pickle , index):\n",
    "    oldId = pickle[index].split(',')[0].split('/')[6].split('_')[0]\n",
    "    seqDict = dict()\n",
    "    indexList = []\n",
    "    sequence = pickle[index].split(',')[0].split('/')[6].split('_')[1].split('.')[0]\n",
    "    seqDict[int(sequence)] = str(pickle[index].split(',')[1])\n",
    "    indexList.append(index)\n",
    "    index+=1\n",
    "    #print index\n",
    "    while (index < len(pickle) and (pickle[index].split(',')[0].split('/')[6].split('_')[0] == oldId)):\n",
    "        sequence = pickle[index].split(',')[0].split('/')[6].split('_')[1].split('.')[0]\n",
    "        seqDict[int(sequence)] = str(pickle[index].split(',')[1])\n",
    "        indexList.append(index)\n",
    "        index += 1\n",
    "    seqDict['indexList'] = indexList\n",
    "    return seqDict , oldId , index\n",
    "    \n",
    "def get_VID_ANP_List(pickle):\n",
    "    megaDict = dict()\n",
    "    i = 0\n",
    "    print len(pickle)\n",
    "    while i < len(pickle):           \n",
    "        subDict , postId , i = get_vid_senti(pickle , i)\n",
    "        megaDict[int(postId)] = subDict\n",
    "    return megaDict\n",
    "\n",
    "\n",
    "def pruneMegaDict(megadict , filterindices):\n",
    "    filteredList = dict()\n",
    "    for entry in megadict:\n",
    "        commns = set(megadict[entry]['indexList']).intersection(filterindices)\n",
    "        if len(commns) >= 4:\n",
    "            filteredList[entry] = megadict[entry]\n",
    "    return filteredList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_list =[]\n",
    "with open(sampled_img_list) as f:\n",
    "    image_list = f.readlines()\n",
    "\n",
    "true_labels = []\n",
    "for line in image_list:\n",
    "    label = line.split('/')[5]\n",
    "    true_labels.append(label)\n",
    "    \n",
    "# f = open(ClassFile ,'r')\n",
    "# sentibankClasses = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "f = open(ClassFile ,'r')\n",
    "sentibankClasses = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentibank_probs = np.zeros(len(sentibank_baseline))\n",
    "detected_labels = []\n",
    "for i in range(sentibank_probs.shape[0]):\n",
    "    sentibank_probs[i] = sentibank_baseline[i].max()\n",
    "    detected_labels.append(str(sentibankClasses[np.argmax(sentibank_baseline[i])]))\n",
    "    \n",
    "print np.median(sentibank_probs), sentibank_probs.var(), sentibank_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxVecs = np.zeros(len(vineProbs))\n",
    "for i in range(len(vineProbs)):\n",
    "    maxVecs[i] = np.max(vineProbs[i])\n",
    "print np.mean(maxVecs) , np.median(maxVecs) , np.var(maxVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curatedProbs_index = []\n",
    "for i in range(len(vineProbs)):\n",
    "    if (vineProbs[i].max() > (np.mean(maxVecs) + np.var(maxVecs) )):\n",
    "        curatedProbs_index.append(i)\n",
    "\n",
    "        \n",
    "print len(curatedProbs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vineList = readLists(vineANPs)\n",
    "objList = readLists(imageNetObjs)\n",
    "print curatedProbs_index[5]\n",
    "print objList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vineList[0].split(',')[0].split('/')[6].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idList, anpList = get_ANP_ID_List(vineList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print idList[1000] , anpList [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentidict0 = readSentiments()\n",
    "sentidict = readRevisedSentiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(sentidict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = open('sentibankANP.csv', 'wb')\n",
    "# for k,v in sentidict0.iteritems():\n",
    "#     f.write(k+','+str(v)+'\\n')\n",
    "#     #print k , v\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "megaDict = get_VID_ANP_List(vineList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#megaDict[1281867050901532672]['indexList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredDict = pruneMegaDict(megaDict, curatedProbs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(filteredDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(megaDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postIds = []\n",
    "for line in vineList:\n",
    "    arr = line.split('/')\n",
    "    i = int(arr[6].split('_')[0])\n",
    "    postIds.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_counts = Counter(postIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(letter_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectors for filtered videos\n",
    "senti_matrix = np.zeros((len(filteredDict),6))\n",
    "print senti_matrix.shape\n",
    "postList = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "misses = 0\n",
    "postIdFilteredList = []\n",
    "for entry in filteredDict:\n",
    "    postIdFilteredList.append(entry)\n",
    "    for j in range(1,7):\n",
    "        if j in filteredDict[entry]:\n",
    "            senti_matrix[i][j-1] = sentidict[filteredDict[entry][j]] if (filteredDict[entry][j] in sentidict) else 0.0\n",
    "        else:\n",
    "            senti_matrix[i][j-1] = 0.0\n",
    "            misses += 1\n",
    "    i += 1\n",
    "print misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get all post meta data \n",
    "dirs,files = walkLevel1Dir(root)\n",
    "visited = getVisited(visitedList)\n",
    "selectedPosts = []\n",
    "allPosts = []\n",
    "\n",
    "for d in dirs:\n",
    "    if d in visited:\n",
    "        dataRoot = root + d\n",
    "        popular = getPopularFile(dataRoot)\n",
    "        rec = getRecords(popular)\n",
    "        allPosts += rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(allPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredPosts = dict()\n",
    "for k in filteredDict:\n",
    "    for post in allPosts:\n",
    "        if post['postId'] == k:\n",
    "            filteredPosts[k] = post\n",
    "print len(filteredPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "rand_smpl = [ allPosts[i] for i in sorted(random.sample(xrange(len(allPosts)), len(filteredPosts))) ]\n",
    "print len(rand_smpl)\n",
    "\n",
    "allLikes = []\n",
    "allReposts = []\n",
    "allLoops = []\n",
    "for rec in rand_smpl:\n",
    "    allLikes.append(rec['likes']['count'])\n",
    "    allReposts.append(rec['reposts']['count'])\n",
    "    allLoops.append(rec['loops']['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filLikes = []\n",
    "filReposts = []\n",
    "filLoops = []\n",
    "filPosts = []\n",
    "for k in filteredPosts:\n",
    "    filLikes.append(np.log(filteredPosts[k]['likes']['count']+1))\n",
    "    filReposts.append(np.log(filteredPosts[k]['reposts']['count']+1))\n",
    "    filLoops.append(np.log(filteredPosts[k]['loops']['count']+1))\n",
    "    filPosts.append(filteredPosts[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.hist(filLikes, 400, normed=1,histtype='step', cumulative=True,linewidth = 3.0,linestyle='dashed')\n",
    "plt.hist(filReposts, 400, normed=1,histtype='step', cumulative=True,linewidth = 3.0)\n",
    "plt.title(\"CDF for Likes and Repost\", fontsize = 25)\n",
    "plt.xlabel(\"Likes/repost count (Log scale)\",fontsize = 25)\n",
    "plt.ylabel(\"CDF\",fontsize = 25)\n",
    "plt.legend(['Likes' , 'Reposts'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortedWRTLikesPosts = [x for (y,x) in sorted(zip(filLikes,filPosts))]\n",
    "sortedWRTRepostsPosts = [x for (y,x) in sorted(zip(filReposts,filPosts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(filLikes) , np.max(filLikes) , np.min(filReposts) , np.max(filReposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.median(filLikes)\n",
    "print np.median(filReposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.corrcoef(filReposts,filLoops)\n",
    "print np.corrcoef(filLikes,filLoops)\n",
    "print np.corrcoef(filReposts,filLikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "clf = LinearSVR()\n",
    "\n",
    "clf.fit(senti_matrix, filReposts) \n",
    "print clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 50013\n",
    "print vineList[num]\n",
    "print objList[num]\n",
    "print len(megaDict)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "img = cv2.imread(vineList[num].split(',')[0],1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print senti_matrix[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq\n",
    "from scipy.spatial.distance import minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroid_list = []\n",
    "id_list = []\n",
    "for i in range(1 , 12):\n",
    "    centroids,_ = kmeans(senti_matrix,i)\n",
    "    idx,_ = vq(senti_matrix,centroids)\n",
    "    centroid_list.append(centroids)\n",
    "    id_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSE_values = np.zeros(len(id_list))\n",
    "for i in range(len(centroid_list)):\n",
    "    for j in range(len(centroid_list[i])):\n",
    "        vecs = senti_matrix[id_list[i]==j,:]\n",
    "        #print vecs.shape\n",
    "        cent = centroid_list[i][j]\n",
    "        SSE_1 = 0.0\n",
    "        for vec in vecs:\n",
    "            SSE_1 = SSE_1 + minkowski(vec,cent,2)\n",
    "        SSE_values[j] = SSE_values[j] + SSE_1\n",
    "for i in range(len(SSE_values)):\n",
    "    SSE_values[i] = SSE_values[i]/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "x = np.arange(1,11)\n",
    "plt.plot(x,SSE_values[:10] ,linewidth = 3.0)\n",
    "plt.xlabel(\"Number of Clusters\", fontsize = 25)\n",
    "plt.ylabel(\"Mean Minkowski distance from cluster centroids\", fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print SSE_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(senti_matrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print U.shape,  np.diag(s).shape, V.shape\n",
    "print np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(senti_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Modes = np.dot(U,np.diag(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexlist = np.argsort(np.linalg.norm(Modes,axis=1))\n",
    "sorted_modes = Modes[indexlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Sentiment transition graphs for all cluster centroids\", fontsize = 25)\n",
    "plt.plot(sorted_modes[0], linewidth = 2.0)\n",
    "plt.plot(sorted_modes[1], linewidth = 2.0)\n",
    "plt.plot(sorted_modes[2], linewidth = 2.0)\n",
    "plt.plot(sorted_modes[3], linewidth = 2.0)\n",
    "plt.plot(sorted_modes[4], linewidth = 2.0 )\n",
    "plt.plot(sorted_modes[5], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[6], linewidth = 2.0 )\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Cluster 1', 'Cluster 2' , 'Cluster 3' \n",
    "            ,'Cluster 4'\n",
    "            ,'Cluster 5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroid_final,_ = kmeans(senti_matrix,5)\n",
    "idx_final,_ = vq(senti_matrix,centroid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs1 = senti_matrix[idx_final==0,:]\n",
    "vecs2 = senti_matrix[idx_final==1,:]\n",
    "vecs3 = senti_matrix[idx_final==2,:]\n",
    "vecs4 = senti_matrix[idx_final==3,:]\n",
    "vecs5 = senti_matrix[idx_final==4,:]\n",
    "vecs6 = senti_matrix[idx_final==5,:]\n",
    "vecs7 = senti_matrix[idx_final==6,:]\n",
    "#senti_matrix[idx_final==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(vecs1)\n",
    "print len(vecs2)\n",
    "print len(vecs3)\n",
    "print len(vecs4)\n",
    "print len(vecs5)\n",
    "print len(vecs6)\n",
    "print len(vecs7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print centroid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.title(\"Sentiment transition graphs for all cluster centroids\", fontsize = 25)\n",
    "plt.plot(centroid_final[0], linewidth = 2.0)\n",
    "plt.plot(centroid_final[1], linewidth = 2.0)\n",
    "plt.plot(centroid_final[2], linewidth = 2.0)\n",
    "plt.plot(centroid_final[3], linewidth = 2.0)\n",
    "plt.plot(centroid_final[4], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[5], linewidth = 2.0 )\n",
    "# plt.plot(centroid_final[6], linewidth = 2.0 )\n",
    "\n",
    "plt.xlabel(\"nth second into the video\", fontsize = 25)\n",
    "plt.ylabel(\"Sentiment\", fontsize = 25)\n",
    "plt.legend(['Cluster 1', 'Cluster 2' , 'Cluster 3' \n",
    "            ,'Cluster 4'\n",
    "            ,'Cluster 5'\n",
    "            #,'Cluster 6'\n",
    "            #,'Cluster 7'\n",
    "            \n",
    "           ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filterCluster(postids , posts , postDict, clusterMems , clusterId ):\n",
    "    plist = []\n",
    "    ANP_list = []\n",
    "    selective = clusterMems == clusterId\n",
    "    ids = [i for (i, v) in zip(postids, selective) if v]\n",
    "    for postid in ids:\n",
    "        plist.append(posts[postid])\n",
    "        ANP_list.append(postDict[postid])\n",
    "    return plist , ANP_list\n",
    "\n",
    "def likesRepostsLoops(cluster):\n",
    "    likes = np.zeros(len(cluster))\n",
    "    reposts = np.zeros(len(cluster))\n",
    "    loops = np.zeros(len(cluster))\n",
    "    for i in range(len(cluster)):\n",
    "        likes[i] = np.log(cluster[i]['likes']['count']+1)\n",
    "        reposts[i] = np.log(cluster[i]['reposts']['count']+1)\n",
    "        loops[i] = np.log(cluster[i]['loops']['count']+1)\n",
    "    return likes, reposts, loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster1 , ANP_cluster1 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,0)\n",
    "print len(cluster1)\n",
    "cluster2 , ANP_cluster2 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,1)\n",
    "print len(cluster2)\n",
    "cluster3 , ANP_cluster3 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,2)\n",
    "print len(cluster3)\n",
    "cluster4 , ANP_cluster4 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,3)\n",
    "print len(cluster4)\n",
    "cluster5 , ANP_cluster5 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,idx_final,4)\n",
    "print len(cluster5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likesC1 , repostsC1 , loopsC1 = likesRepostsLoops(cluster1)\n",
    "likesC2 , repostsC2 , loopsC2 = likesRepostsLoops(cluster2)\n",
    "likesC3 , repostsC3 , loopsC3 = likesRepostsLoops(cluster3)\n",
    "likesC4 , repostsC4 , loopsC4 = likesRepostsLoops(cluster4)\n",
    "likesC5 , repostsC5 , loopsC5 = likesRepostsLoops(cluster5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# likesC1 = likesC1/len(senti_matrix)\n",
    "# likesC2 = likesC2/len(senti_matrix)\n",
    "# likesC3 = likesC3/len(senti_matrix)\n",
    "# likesC4 = likesC4/len(senti_matrix)\n",
    "# likesC5 = likesC1/len(senti_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1 = np.random.choice(len(likesC1), 50)\n",
    "sampledLikesC1 = likesC1[c1]\n",
    "\n",
    "c2 = np.random.choice(len(likesC2), 50)\n",
    "sampledLikesC2 = likesC2[c2]\n",
    "\n",
    "c3 = np.random.choice(len(likesC3), 50)\n",
    "sampledLikesC3 = likesC3[c3]\n",
    "\n",
    "c4 = np.random.choice(len(likesC4), 50)\n",
    "sampledLikesC4 = likesC4[c4]\n",
    "\n",
    "c5 = np.random.choice(len(likesC5), 50)\n",
    "sampledLikesC5 = likesC5[c5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Numbers for Cluster C1: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC1),np.mean(repostsC1),np.median(sampledLikesC1),np.mean(sampledLikesC1),np.median(loopsC1),np.mean(loopsC1))\n",
    "\n",
    "print \"Numbers for Cluster C2: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC2),np.mean(repostsC2),np.median(sampledLikesC2),np.mean(sampledLikesC2),np.median(loopsC2),np.mean(loopsC2))\n",
    "\n",
    "print \"Numbers for Cluster C3: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC3),np.mean(repostsC3),np.median(sampledLikesC3),np.mean(sampledLikesC3),np.median(loopsC3),np.mean(loopsC3))\n",
    "\n",
    "print \"Numbers for Cluster C4: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC4),np.mean(repostsC4),np.median(sampledLikesC4),np.mean(sampledLikesC4),np.median(loopsC4),np.mean(loopsC4))\n",
    "\n",
    "print \"Numbers for Cluster C5: Median Reposts: %f , Mean Reposts: %f , Median Likes: %f , \\\n",
    "Mean Likes: %f , Median Loops: %f , Mean Loops \\\n",
    "%f\"%(np.median(repostsC5),np.mean(repostsC5),np.median(sampledLikesC5),np.mean(sampledLikesC5),np.median(loopsC5),np.mean(loopsC5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVR()\n",
    "\n",
    "clf.fit(vecs3, repostsC3) \n",
    "print clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15, 10)\n",
    "# plt.hist(likesC1, 400, normed=1,histtype='step', cumulative=False,linewidth = 3.0,linestyle='dashed')\n",
    "# plt.hist(likesC2, 400, normed=1,histtype='step', cumulative=False,linewidth = 3.0)\n",
    "# plt.hist(likesC3, 400, normed=1,histtype='step', cumulative=False, linewidth = 3.0,linestyle='dotted')\n",
    "# plt.hist(likesC4, 400, normed=1,histtype='step', cumulative=False, linewidth = 3.0,linestyle='dotted')\n",
    "# plt.hist(likesC5, 400, normed=1,histtype='step', cumulative=True, linewidth = 3.0,linestyle='dotted')\n",
    "# plt.title(\"CDF for Likes for the 4 sentiment clusters \", fontsize = 25)\n",
    "# plt.xlabel(\"Likes/repost count\",fontsize = 25)\n",
    "# plt.ylabel(\"CDF\",fontsize = 25)\n",
    "# plt.legend(['Likes C1', 'Likes C2' , 'Likes C3' , 'Likes C4'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotDist(RV, samples, b):\n",
    "    N = samples \n",
    "    n = b\n",
    "    s = np.random.normal(size=N)   # generate your data sample with N elements\n",
    "    p1, x1 = np.histogram(RV, bins=n) # bin it into n = N/10 bins\n",
    "    xs1 = x1[:-1] + (x1[1] - x1[0])/2\n",
    "    f1 = UnivariateSpline(xs1, p1)\n",
    "    plt.plot(xs1 , f1(xs1) ,'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plotDist(sampledLikesC1 , 1000 , 50)\n",
    "plotDist(sampledLikesC2 , 1000 , 50)\n",
    "plotDist(sampledLikesC3 , 1000 , 50)\n",
    "plotDist(sampledLikesC4 , 1000 , 50)\n",
    "plotDist(sampledLikesC5 , 1000 , 50)\n",
    "plt.legend(['Reposts C1', 'Reposts C2' , 'Reposts C3' , 'Reposts C4', 'Reposts C4'])\n",
    "plt.xlabel(\"Like count per video per cluster in Log scale\",fontsize = 25)\n",
    "plt.ylabel(\"Frequency \",fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15, 10)\n",
    "# N = 1000\n",
    "# n = N/50\n",
    "# s = np.random.normal(size=N)   # generate your data sample with N elements\n",
    "# p1, x1 = np.histogram(repostsC1, bins=n) # bin it into n = N/10 bins\n",
    "# p2, x2 = np.histogram(repostsC2, bins=n) # bin it into n = N/10 bins\n",
    "# p3, x3 = np.histogram(repostsC3, bins=n) # bin it into n = N/10 bins\n",
    "# p4, x4 = np.histogram(repostsC4, bins=n) # bin it into n = N/10 bins\n",
    "\n",
    "# xs1 = x1[:-1] + (x1[1] - x1[0])/2   # convert bin edges to centers\n",
    "# xs2 = x2[:-1] + (x2[1] - x2[0])/2   # convert bin edges to centers\n",
    "# xs3 = x3[:-1] + (x3[1] - x3[0])/2   # convert bin edges to centers\n",
    "# xs4 = x4[:-1] + (x4[1] - x4[0])/2   # convert bin edges to centers\n",
    "\n",
    "# f1 = UnivariateSpline(xs1, p1)\n",
    "# f2 = UnivariateSpline(xs2, p2)\n",
    "# f3 = UnivariateSpline(xs3, p3)\n",
    "# f4 = UnivariateSpline(xs4, p4)\n",
    "\n",
    "# plt.plot(xs1, f1(xs1) ,'.-')\n",
    "# plt.plot(xs2, f2(xs2) ,'.-')\n",
    "# plt.plot(xs3, f3(xs3) ,'.-')\n",
    "# plt.plot(xs4, f4(xs4) ,'.-')\n",
    "\n",
    "# plt.legend(['Reposts C1', 'Reposts C2' , 'Reposts C3' , 'Reposts C4'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.interpolate import UnivariateSpline\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15, 10)\n",
    "# N = 1000\n",
    "# n = N/10\n",
    "# p, x1 = np.histogram(repostsC1, bins=n) # bin it into n = N/10 bins\n",
    "# p, x2 = np.histogram(repostsC2, bins=n) # bin it into n = N/10 bins\n",
    "# p, x3 = np.histogram(repostsC3, bins=n) # bin it into n = N/10 bins\n",
    "# x1 = x1[:-1] + (x1[1] - x1[0])/2   # convert bin edges to centers\n",
    "# x2 = x2[:-1] + (x2[1] - x2[0])/2   # convert bin edges to centers\n",
    "# x3 = x3[:-1] + (x3[1] - x3[0])/2   # convert bin edges to centers\n",
    "# f = UnivariateSpline(x, p, s=n)\n",
    "# plt.plot(x1, f(x1))\n",
    "# plt.plot(x2, f(x2))\n",
    "# plt.plot(x3, f(x3))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15, 10)\n",
    "# N = 1000\n",
    "# n = N/10\n",
    "# p, x1 = np.histogram(likesC1, bins=n) # bin it into n = N/10 bins\n",
    "# p, x2 = np.histogram(likesC2, bins=n) # bin it into n = N/10 bins\n",
    "# p, x3 = np.histogram(likesC3, bins=n) # bin it into n = N/10 bins\n",
    "# #x1 = x1[:-1] + (x1[1] - x1[0])/2   # convert bin edges to centers\n",
    "# #x2 = x2[:-1] + (x2[1] - x2[0])/2   # convert bin edges to centers\n",
    "# #x3 = x3[:-1] + (x3[1] - x3[0])/2   # convert bin edges to centers\n",
    "# f = UnivariateSpline(x, p, s=n)\n",
    "# plt.plot(x1, f(x1))\n",
    "# plt.plot(x2, f(x2))\n",
    "# plt.plot(x3, f(x3))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from numpy import linspace,exp\n",
    "# from numpy.random import randn\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.interpolate import UnivariateSpline\n",
    "# x = linspace(-3, 3, 100)\n",
    "# y = exp(-x**2) + randn(100)/10\n",
    "# s = UnivariateSpline(x, y, s=1)\n",
    "# xs = linspace(-3, 3, 1000)\n",
    "# ys = s(xs)\n",
    "# plt.plot(x, y, '.-')\n",
    "# plt.plot(xs, ys)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15, 10)\n",
    "# plt.hist(repostsC1, 400, normed=1,histtype='step', cumulative=True,linewidth = 3.0,linestyle='dashed')\n",
    "# plt.hist(repostsC2, 400, normed=1,histtype='step', cumulative=True,linewidth = 3.0)\n",
    "# plt.hist(repostsC3, 400, normed=1,histtype='step', cumulative=True, linewidth = 3.0,linestyle='dotted')\n",
    "# #plt.hist(repostsC4, 400, normed=1,histtype='step', cumulative=True, linewidth = 3.0,linestyle='dotted')\n",
    "# plt.title(\"CDF for Reposts for the 4 sentiment clusters \", fontsize = 25)\n",
    "# plt.xlabel(\"Repost count\",fontsize = 25)\n",
    "# plt.ylabel(\"CDF\",fontsize = 25)\n",
    "# plt.legend(['Reposts C1', 'Reposts C2' , 'Reposts C3' , 'Reposts C4'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print np.std(repostsC1) , np.std(repostsC2), np.std(repostsC3), np.std(repostsC4)#, np.std(repostsC5)\n",
    "\n",
    "# print np.std(likesC1) , np.std(likesC2), np.std(likesC3), np.std(likesC4)#, np.std(likesC5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print senti_matrix[52]\n",
    "# print postIdFilteredList[52]\n",
    "# print filteredDict[postIdFilteredList[52]]\n",
    "# print filteredPosts[postIdFilteredList[52]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "num = 155\n",
    "url = cluster1[num]['videoDashUrl'].strip().split('/')[-1].split('?')[0]\n",
    "print  cluster1[num]['videoDashUrl']\n",
    "print ANP_cluster1[num]\n",
    "\n",
    "#print url\n",
    "videoPath = \"/datasets/sagarj/vineData/Dataset/Videos/\" + url\n",
    "print videoPath\n",
    "\n",
    "\n",
    "video = io.open(videoPath, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFeatures(lines):\n",
    "    ids = []\n",
    "    features = []\n",
    "    feats = []\n",
    "    \n",
    "    components = lines[0].strip().split(',')\n",
    "    ID = components[0].split('/')[1].strip().split('_')[0]\n",
    "    \n",
    "    for line in lines:\n",
    "        components = line.strip().split(',')\n",
    "        postId = components[0].split('/')[1].strip().split('_')[0]\n",
    "        if postId != ID:\n",
    "            ids.append(postId)\n",
    "            ID = postId\n",
    "            #Take median of all frames in a vine for all the attributes\n",
    "            features.append(np.median(feats,0))\n",
    "            feats[:] = []\n",
    "            feats.append([float(x) for x in components[1:]]) \n",
    "        else :\n",
    "            feats.append([float(x) for x in components[1:]]) \n",
    "            \n",
    "    return features, ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(color_features) as g:\n",
    "    featureLines = g.readlines()\n",
    "print len(featureLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureLines[6].strip().split('|')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features , posts = readFeatures(featureLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(posts) , len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_features = np.asarray(features)\n",
    "print aes_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aes_centroid_list = []\n",
    "# aes_id_list = []\n",
    "# for i in range(1 , 12):\n",
    "#     centroids,_ = kmeans(aes_features,i)\n",
    "#     idx,_ = vq(aes_features,centroids)\n",
    "#     aes_centroid_list.append(centroids)\n",
    "#     aes_id_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# SSE_values = np.zeros(len(aes_id_list))\n",
    "# for i in range(len(aes_centroid_list)):\n",
    "#     for j in range(len(aes_centroid_list[i])):\n",
    "#         vecs = aes_features[aes_id_list[i]==j,:]\n",
    "#         #print vecs.shape\n",
    "#         cent = aes_centroid_list[i][j]\n",
    "#         SSE_1 = 0.0\n",
    "#         for vec in vecs:\n",
    "#             SSE_1 = SSE_1 + minkowski(vec,cent,2)\n",
    "#         SSE_values[j] = SSE_values[j] + SSE_1\n",
    "# for i in range(len(SSE_values)):\n",
    "#     SSE_values[i] = SSE_values[i]/(i+1)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(20, 15)\n",
    "# plt.rc('xtick', labelsize=20) \n",
    "# plt.rc('ytick', labelsize=20)\n",
    "# x = np.arange(1,11)\n",
    "# plt.plot(x,SSE_values[:10])\n",
    "# plt.xlabel(\"Number of Clusters\", fontsize = 25)\n",
    "# plt.ylabel(\"Minkowski distance from cluster centroids\", fontsize = 25)\n",
    "# plt.title(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aes_centroid_final,_ = kmeans(aes_features,4)\n",
    "# aes_idx_final,_ = vq(aes_features,aes_centroid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aes_cluster1 , ANP_cluster1 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,aes_idx_final,0)\n",
    "# print len(aes_cluster1)\n",
    "# aes_cluster2 , ANP_cluster2 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,aes_idx_final,1)\n",
    "# print len(aes_cluster2)\n",
    "# aes_cluster3 , ANP_cluster3 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,aes_idx_final,2)\n",
    "# print len(aes_cluster3)\n",
    "# aes_cluster4 , ANP_cluster4 = filterCluster(postIdFilteredList,filteredPosts,filteredDict,aes_idx_final,3)\n",
    "# print len(aes_cluster4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import io\n",
    "# import base64\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# num = 50\n",
    "# url = aes_cluster4[num]['videoDashUrl'].strip().split('/')[-1].split('?')[0]\n",
    "# print  aes_cluster4[num]['videoDashUrl']\n",
    "# print ANP_cluster4[num]\n",
    "# #print url\n",
    "# videoPath = \"/datasets/sagarj/vineData/Dataset/Videos/\" + url\n",
    "# #print videoPath\n",
    "\n",
    "\n",
    "# video = io.open(videoPath, 'r+b').read()\n",
    "# encoded = base64.b64encode(video)\n",
    "# HTML(data='''<video alt=\"test\" controls>\n",
    "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "#              </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print aes_centroid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print cluster1[0]['postId']\n",
    "print posts.index(str(cluster1[0]['postId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAesFeatures(cluster, posts , numpyFeatures):\n",
    "    indices = []\n",
    "    for c in cluster:\n",
    "        i = posts.index(str(c['postId']))\n",
    "        if i:\n",
    "            indices.append(i)\n",
    "    features = numpyFeatures[indices]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Aes1 = getAesFeatures(cluster1, posts , aes_features)\n",
    "Aes2 = getAesFeatures(cluster2, posts , aes_features)\n",
    "Aes3 = getAesFeatures(cluster3, posts , aes_features)\n",
    "Aes4 = getAesFeatures(cluster4, posts , aes_features)\n",
    "Aes5 = getAesFeatures(cluster5, posts , aes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Aes1.shape\n",
    "print Aes2.shape\n",
    "print Aes3.shape\n",
    "print Aes4.shape\n",
    "print Aes5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print Aes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Aes2[:,9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "plotDist(Aes1[:,10]/len(Aes1) , 1000 , 50)\n",
    "plotDist(Aes2[:,10]/len(Aes2) , 1000 , 50)\n",
    "plotDist(Aes3[:,10]/len(Aes3) , 1000 , 50)\n",
    "plotDist(Aes4[:,10]/len(Aes4) , 1000 , 50)\n",
    "plotDist(Aes5[:,10]/len(Aes5) , 1000 , 50)\n",
    "plt.legend(['Reposts C1', 'Reposts C2' , 'Reposts C3' , 'Reposts C4', 'Reposts C5'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
