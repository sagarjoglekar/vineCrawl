{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import timeit\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from load import mnist\n",
    "from utils import tile_raster_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start-snippet-1\n",
    "class RBM(object):\n",
    "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RBM constructor. Defines the parameters of the model along with\n",
    "        basic operations for inferring hidden from visible (and vice-versa),\n",
    "        as well as for performing CD updates.\n",
    "\n",
    "        :param input: None for standalone RBMs or symbolic variable if RBM is\n",
    "        part of a larger graph.\n",
    "\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :param W: None for standalone RBMs or symbolic variable pointing to a\n",
    "        shared weight matrix in case RBM is part of a DBN network; in a DBN,\n",
    "        the weights are shared between RBMs and layers of a MLP\n",
    "\n",
    "        :param hbias: None for standalone RBMs or symbolic variable pointing\n",
    "        to a shared hidden units bias vector in case RBM is part of a\n",
    "        different network\n",
    "\n",
    "        :param vbias: None for standalone RBMs or a symbolic variable\n",
    "        pointing to a shared visible units bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        if numpy_rng is None:\n",
    "            # create a number generator\n",
    "            numpy_rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        if theano_rng is None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        if W is None:\n",
    "            # W is initialized with `initial_W` which is uniformely\n",
    "            # sampled from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible)) the output of uniform if\n",
    "            # converted using asarray to dtype theano.config.floatX so\n",
    "            # that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            # theano shared variables for weights and biases\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if hbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            hbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='hbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if vbias is None:\n",
    "            # create shared variable for visible units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='vbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "\n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.theano_rng = theano_rng\n",
    "        # **** WARNING: It is not a good idea to put things in this list\n",
    "        # other than shared variables created in this function.\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "        # end-snippet-1\n",
    "\n",
    "    def free_energy(self, v_sample):\n",
    "        ''' Function to compute the free energy '''\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    def propup(self, vis):\n",
    "        '''This function propagates the visible units activation upwards to\n",
    "        the hidden units\n",
    "\n",
    "        Note that we return also the pre-sigmoid activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        ''' This function infers state of hidden units given visible units '''\n",
    "        # compute the activation of the hidden units given a sample of\n",
    "        # the visibles\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        # get a sample of the hiddens given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        '''This function propagates the hidden units activation downwards to\n",
    "        the visible units\n",
    "\n",
    "        Note that we return also the pre_sigmoid_activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        ''' This function infers state of visible units given hidden units '''\n",
    "        # compute the activation of the visible given the hidden sample\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        # get a sample of the visible given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the hidden state'''\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the visible state'''\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    # start-snippet-2\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        \"\"\"This functions implements one step of CD-k or PCD-k\n",
    "\n",
    "        :param lr: learning rate used to train the RBM\n",
    "\n",
    "        :param persistent: None for CD. For PCD, shared variable\n",
    "            containing old state of Gibbs chain. This must be a shared\n",
    "            variable of size (batch size, number of hidden units).\n",
    "\n",
    "        :param k: number of Gibbs steps to do in CD-k/PCD-k\n",
    "\n",
    "        Returns a proxy for the cost and the updates dictionary. The\n",
    "        dictionary contains the update rules for weights and biases but\n",
    "        also an update of the shared variable used to store the persistent\n",
    "        chain, if one is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive phase\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "\n",
    "        # decide how to initialize persistent chain:\n",
    "        # for CD, we use the newly generate hidden sample\n",
    "        # for PCD, we initialize from the old state of the chain\n",
    "        if persistent is None:\n",
    "            chain_start = ph_sample\n",
    "        else:\n",
    "            chain_start = persistent\n",
    "        # end-snippet-2\n",
    "        # perform actual negative phase\n",
    "        # in order to implement CD-k/PCD-k we need to scan over the\n",
    "        # function that implements one gibbs step k times.\n",
    "        # Read Theano tutorial on scan for more information :\n",
    "        # http://deeplearning.net/software/theano/library/scan.html\n",
    "        # the scan will return the entire Gibbs chain\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            # the None are place holders, saying that\n",
    "            # chain_start is the initial state corresponding to the\n",
    "            # 6th output\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k\n",
    "        )\n",
    "        # start-snippet-3\n",
    "        # determine gradients on RBM parameters\n",
    "        # note that we only need the sample at the end of the chain\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(\n",
    "            self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "        # end-snippet-3 start-snippet-4\n",
    "        # constructs the update dictionary\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(\n",
    "                lr,\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        if persistent:\n",
    "            # Note that this works only if persistent is a shared variable\n",
    "            updates[persistent] = nh_samples[-1]\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(updates,\n",
    "                                                           pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "        # end-snippet-4\n",
    "\n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n",
    "\n",
    "        # index of bit i in expression p(x_i | x_{\\i})\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n",
    "        # the result to xi_flip, instead of working in place on xi.\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n",
    "                                                            fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n",
    "        \"\"\"Approximation to the reconstruction error\n",
    "\n",
    "        Note that this function requires the pre-sigmoid activation as\n",
    "        input.  To understand why this is so you need to understand a\n",
    "        bit about how Theano works. Whenever you compile a Theano\n",
    "        function, the computational graph that you pass as input gets\n",
    "        optimized for speed and stability.  This is done by changing\n",
    "        several parts of the subgraphs with others.  One such\n",
    "        optimization expresses terms of the form log(sigmoid(x)) in\n",
    "        terms of softplus.  We need this optimization for the\n",
    "        cross-entropy since sigmoid of numbers larger than 30. (or\n",
    "        even less then that) turn to 1. and numbers smaller than\n",
    "        -30. turn to 0 which in terms will force theano to compute\n",
    "        log(0) and therefore we will get either -inf or NaN as\n",
    "        cost. If the value is expressed in terms of softplus we do not\n",
    "        get this undesirable behaviour. This optimization usually\n",
    "        works fine, but here we have a special case. The sigmoid is\n",
    "        applied inside the scan op, while the log is\n",
    "        outside. Therefore Theano will only see log(scan(..)) instead\n",
    "        of log(sigmoid(..)) and will not apply the wanted\n",
    "        optimization. We can not go and replace the sigmoid in scan\n",
    "        with something else also, because this only needs to be done\n",
    "        on the last step. Therefore the easiest and more efficient way\n",
    "        is to get also the pre-sigmoid activation as an output of\n",
    "        scan, and apply both the log and sigmoid outside scan such\n",
    "        that Theano can catch and optimize the expression.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_rbm(learning_rate=0.1, training_epochs=15,\n",
    "             dataset='mnist.pkl.gz', batch_size=20,\n",
    "             n_chains=20, n_samples=10, output_folder='rbm_plots',\n",
    "             n_hidden=500):\n",
    "    \"\"\"\n",
    "    Demonstrate how to train and afterwards sample from it using Theano.\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :param learning_rate: learning rate used for training the RBM\n",
    "\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :param dataset: path the the pickled dataset\n",
    "\n",
    "    :param batch_size: size of a batch used to train the RBM\n",
    "\n",
    "    :param n_chains: number of parallel Gibbs chains to be used for sampling\n",
    "\n",
    "    :param n_samples: number of samples to plot for each chain\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    trX, teX, trY, teY  = mnist(onehot=True)\n",
    "    \n",
    "    train_set_x = theano.shared(numpy.asarray(trX, dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_x = theano.shared(numpy.asarray(trX, dtype=theano.config.floatX),borrow=True)\n",
    "    train_set_y = theano.shared(numpy.asarray(trY, dtype='int32'),borrow=True)\n",
    "    test_set_y = theano.shared(numpy.asarray(teY, dtype='int32'),borrow=True)\n",
    "    \n",
    "    print train_set_x.shape\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    print str(n_train_batches)\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    # initialize storage for the persistent chain (state = hidden\n",
    "    # layer of chain)\n",
    "    persistent_chain = theano.shared(numpy.zeros((batch_size, n_hidden),\n",
    "                                                 dtype=theano.config.floatX),\n",
    "                                     borrow=True)\n",
    "\n",
    "    # construct the RBM class\n",
    "    rbm = RBM(input=x, n_visible=28 * 28,\n",
    "              n_hidden=n_hidden, numpy_rng=rng, theano_rng=theano_rng)\n",
    "\n",
    "    # get the cost and the gradient corresponding to one step of CD-15\n",
    "    cost, updates = rbm.get_cost_updates(lr=learning_rate,\n",
    "                                         persistent=persistent_chain, k=15)\n",
    "\n",
    "    #################################\n",
    "    #     Training the RBM          #\n",
    "    #################################\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    os.chdir(output_folder)\n",
    "\n",
    "    # start-snippet-5\n",
    "    # it is ok for a theano function to have no output\n",
    "    # the purpose of train_rbm is solely to update the RBM parameters\n",
    "    print str(index * batch_size)\n",
    "    \n",
    "    train_rbm = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print 'Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost)\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "        # Construct image from the weight matrix\n",
    "        image = Image.fromarray(\n",
    "            tile_raster_images(\n",
    "                X=rbm.W.get_value(borrow=True).T,\n",
    "                img_shape=(28, 28),\n",
    "                tile_shape=(10, 10),\n",
    "                tile_spacing=(1, 1)\n",
    "            )\n",
    "        )\n",
    "        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))\n",
    "    # end-snippet-5 start-snippet-6\n",
    "    #################################\n",
    "    #     Sampling from the RBM     #\n",
    "    #################################\n",
    "    # find out the number of test samples\n",
    "    number_of_test_samples = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(number_of_test_samples - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x.get_value(borrow=True)[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    # end-snippet-6 start-snippet-7\n",
    "    plot_every = 1000\n",
    "    # define one step of Gibbs sampling (mf = mean-field) define a\n",
    "    # function that does `plot_every` steps before returning the\n",
    "    # sample for plotting\n",
    "    (\n",
    "        [\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every\n",
    "    )\n",
    "\n",
    "    # add to updates the shared variable that takes care of our persistent\n",
    "    # chain :.\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]})\n",
    "    # construct the function that implements our persistent chain.\n",
    "    # we generate the \"mean field\" activations for plotting and the actual\n",
    "    # samples for reinitializing the state of our persistent chain\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [\n",
    "            vis_mfs[-1],\n",
    "            vis_samples[-1]\n",
    "        ],\n",
    "        updates=updates,\n",
    "        name='sample_fn'\n",
    "    )\n",
    "\n",
    "    # create a space to store the image for plotting ( we need to leave\n",
    "    # room for the tile_spacing as well)\n",
    "    image_data = numpy.zeros(\n",
    "        (29 * n_samples + 1, 29 * n_chains - 1),\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    for idx in xrange(n_samples):\n",
    "        # generate `plot_every` intermediate samples that we discard,\n",
    "        # because successive samples in the chain are too correlated\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print ' ... plotting sample ', idx\n",
    "        image_data[29 * idx:29 * idx + 28, :] = tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    # construct image\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')\n",
    "    # end-snippet-7\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape.0\n",
      "3000\n",
      "Elemwise{mul,no_inplace}.0\n",
      "Training epoch 0, cost is  -93.0207\n",
      "Training epoch 1, cost is  -73.818\n",
      "Training epoch 2, cost is  -70.5175\n",
      "Training epoch 3, cost is  -68.0069\n",
      "Training epoch 4, cost is  -67.6314\n",
      "Training epoch 5, cost is  -67.9866\n",
      "Training epoch 6, cost is  -65.711\n",
      "Training epoch 7, cost is  -63.4478\n",
      "Training epoch 8, cost is  -62.9302\n",
      "Training epoch 9, cost is  -62.7218\n",
      "Training epoch 10, cost is  -62.1705\n",
      "Training epoch 11, cost is  -64.9631\n",
      "Training epoch 12, cost is  -63.132\n",
      "Training epoch 13, cost is  -60.7105\n",
      "Training epoch 14, cost is  -60.9061\n",
      "Training took 59.489906 minutes\n",
      " ... plotting sample  0\n",
      " ... plotting sample  1\n",
      " ... plotting sample  2\n",
      " ... plotting sample  3\n",
      " ... plotting sample  4\n",
      " ... plotting sample  5\n",
      " ... plotting sample  6\n",
      " ... plotting sample  7\n",
      " ... plotting sample  8\n",
      " ... plotting sample  9\n"
     ]
    }
   ],
   "source": [
    "test_rbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
