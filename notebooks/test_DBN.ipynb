{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import timeit\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "import pickle\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from load import mnist\n",
    "from load import faces\n",
    "from utils import tile_raster_images\n",
    "from rbm import RBM\n",
    "from dbn import DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentDir =  os.getcwd();\n",
    "parampickle = currentDir + \"/parametersDBN.pickle\"\n",
    "logPickle = currentDir + \"/errorPickleDBN.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_DBN(finetune_lr=0.01, pretraining_epochs=100,\n",
    "             pretrain_lr=0.1, k=1, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=10):\n",
    "    \"\"\"\n",
    "    Demonstrates how to train and test a Deep Belief Network.\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type finetune_lr: float\n",
    "    :param finetune_lr: learning rate used in the finetune stage\n",
    "    :type pretraining_epochs: int\n",
    "    :param pretraining_epochs: number of epoch to do pretraining\n",
    "    :type pretrain_lr: float\n",
    "    :param pretrain_lr: learning rate to be used during pre-training\n",
    "    :type k: int\n",
    "    :param k: number of Gibbs steps in CD/PCD\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: maximal number of iterations ot run the optimizer\n",
    "    :type dataset: string\n",
    "    :param dataset: path the the pickled dataset\n",
    "    :type batch_size: int\n",
    "    :param batch_size: the size of a minibatch\n",
    "    \"\"\"\n",
    "\n",
    "    #trX, teX, trY, teY  = mnist(onehot=True)\n",
    "    trX, teX, trY, teY  = faces()\n",
    "    \n",
    "    train_set_x = theano.shared(numpy.asarray(trX, dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_x = theano.shared(numpy.asarray(trX, dtype=theano.config.floatX),borrow=True)\n",
    "    train_set_y = theano.shared(numpy.asarray(trY, dtype='int32'),borrow=True)\n",
    "    test_set_y = theano.shared(numpy.asarray(teY, dtype='int32'),borrow=True)\n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    numpy_rng = numpy.random.RandomState(123)\n",
    "    print '... building the model'\n",
    "    logline = \"... building the model\"\n",
    "    f2 = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f2);\n",
    "    f2.close()\n",
    "    \n",
    "    # construct the Deep Belief Network\n",
    "    dbn = DBN(numpy_rng=numpy_rng, n_ins=48 * 48,\n",
    "              hidden_layers_sizes=[1000, 1000, 1000],\n",
    "              n_outs=7)\n",
    "\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print '... getting the pretraining functions'\n",
    "    logline = \"... getting the pretraining functions\"\n",
    "    f2 = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f2);\n",
    "    f2.close()\n",
    "    \n",
    "    pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size,\n",
    "                                                k=k)\n",
    "\n",
    "    print '... pre-training the model'\n",
    "    logline = \"... pre-training the model\"\n",
    "    f2 = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f2);\n",
    "    f2.close()\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "    for i in xrange(dbn.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in xrange(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in xrange(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "            \n",
    "            logline = \"Pre-training layer: \" + str(i) +\"epoch: \" + str(epoch) + \"mean: \" + str(numpy.mean(c))\n",
    "            f2 = open(logPickle, 'a+')\n",
    "            pickle.dump(logline , f2);\n",
    "            f2.close()\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print '... getting the finetuning functions'\n",
    "    logline = \"----Getting Finetuning-----\"\n",
    "    f2 = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f2);\n",
    "    f2.close()\n",
    "    \n",
    "    train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    print '... finetuning the model'\n",
    "    logline = \"------finetuning the model------\"\n",
    "    f2 = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f2);\n",
    "    f2.close()\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "    patience_increase = 2.    # wait this much longer when a new best is\n",
    "                              # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%'\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "                logline = \"epoch: \" + str(epoch) +\"minibatch: \" + str(minibatch_index + 1) + \"validation error: \" + str(this_validation_loss * 100.)\n",
    "                f2 = open(logPickle, 'a+')\n",
    "                pickle.dump(logline , f2);\n",
    "                f2.close()\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "                    logline = \"epoch: \" + str(epoch) +\"minibatch: \" + str(minibatch_index + 1) + \"Test error: \" + str(test_score * 100.)\n",
    "                    f2 = open(logPickle, 'a+')\n",
    "                    pickle.dump(logline , f2);\n",
    "                    f2.close()\n",
    "\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    \n",
    "    logline = \"Optimization complete with best validation score of: \" + str(best_validation_loss * 100.) +\"obtained at iteration: \" + str(i) + \"Test error: \" + str(test_score * 100.)\n",
    "    f2 = open(logPickle, 'a+')\n",
    "    pickle.dump(logline , f2);\n",
    "    f2.close()\n",
    "    \n",
    "    dbn.save_DBN_state(parampickle)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_DBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
