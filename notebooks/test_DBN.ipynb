{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import timeit\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "import pickle\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from load import mnist\n",
    "from load import faces\n",
    "from utils import tile_raster_images\n",
    "from rbm import RBM\n",
    "from dbn import DBN\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.cm as cm\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentDir =  os.getcwd();\n",
    "parampickle = currentDir + \"/parametersDBNV2.pickle\"\n",
    "logPickle = currentDir + \"/errorPickleDBNV2.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX, teX, trY, teY  = faces(onehot=False)\n",
    "#trX, teX, trY, teY  = mnist(onehot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2304)\n",
      "(3589, 2304)\n"
     ]
    }
   ],
   "source": [
    "print trX.shape\n",
    "print teX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrX = trX[:25000]\n",
    "ValX = trX[25000:]\n",
    "TrY = trY[:25000]\n",
    "ValY = trY[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "train_set_x = theano.shared(numpy.asarray(TrX, dtype=theano.config.floatX),borrow=True)\n",
    "test_set_x = theano.shared(numpy.asarray(teX, dtype=theano.config.floatX),borrow=True)\n",
    "train_set_y = theano.shared(numpy.asarray(TrY, dtype=\"int32\"),borrow=True)\n",
    "test_set_y = theano.shared(numpy.asarray(teY, dtype=\"int32\"),borrow=True)\n",
    "\n",
    "val_set_x = theano.shared(numpy.asarray(ValX, dtype=theano.config.floatX),borrow=True)\n",
    "val_set_y = theano.shared(numpy.asarray(ValY, dtype=\"int32\"),borrow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = [(train_set_x, train_set_y), (val_set_x, val_set_y),(test_set_x, test_set_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finetune_lr=0.0005\n",
    "pretraining_epochs=100\n",
    "pretrain_lr=0.005 \n",
    "k=14\n",
    "training_epochs=500\n",
    "batch_size=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "FC layer W matrix size : 2304 X 1400\n",
      "FC layer Bias size : 1400 X 1\n",
      "FC layer W matrix size : 1400 X 1400\n",
      "FC layer Bias size : 1400 X 1\n",
      "FC layer W matrix size : 1400 X 500\n",
      "FC layer Bias size : 500 X 1\n",
      "... getting the pretraining functions\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'n_hidden' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-51203d4ec3f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n\u001b[0;32m     28\u001b[0m                                                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                                                 k=k)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'... pre-training the model'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/work/sagarj/Work/vineCrawl/lib/dbn.py\u001b[0m in \u001b[0;36mpretraining_functions\u001b[1;34m(self, train_set_x, batch_size, k)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;31m# using CD-k here (persisent=None) for training each RBM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[1;31m# TODO: change cost function to reconstruction error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             persistent_chain = theano.shared(numpy.zeros((batch_size, n_hidden),\n\u001b[0m\u001b[0;32m    175\u001b[0m                                                  dtype=theano.config.floatX),\n\u001b[0;32m    176\u001b[0m                                      borrow=True)\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'n_hidden' is not defined"
     ]
    }
   ],
   "source": [
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "# numpy random generator\n",
    "numpy_rng = numpy.random.RandomState(23455)\n",
    "print '... building the model'\n",
    "logline = \"... building the model\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "\n",
    "# construct the Deep Belief Network\n",
    "dbn = DBN(numpy_rng=numpy_rng, n_ins=48 * 48,\n",
    "              hidden_layers_sizes=[1400,1400,500],\n",
    "              n_outs=7)\n",
    "\n",
    "# start-snippet-2\n",
    "#########################\n",
    "# PRETRAINING THE MODEL #\n",
    "\n",
    "#########################\n",
    "print '... getting the pretraining functions'\n",
    "logline = \"... getting the pretraining functions\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size,\n",
    "                                                k=k)\n",
    "\n",
    "print '... pre-training the model'\n",
    "logline = \"... pre-training the model\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "    \n",
    "start_time = timeit.default_timer()\n",
    "## Pre-train layer-wise\n",
    "for i in xrange(dbn.n_layers):\n",
    "    # go through pretraining epochs\n",
    "    for epoch in xrange(pretraining_epochs):\n",
    "        # go through the training set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "        print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "        print numpy.mean(c)\n",
    "            \n",
    "        logline = \"Pre-training layer: \" + str(i) +\"epoch: \" + str(epoch) + \"mean: \" + str(numpy.mean(c))\n",
    "        f2 = open(logPickle, 'a+')\n",
    "        pickle.dump(logline , f2);\n",
    "        f2.close()\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print '... Finished pre-training in :' + str((end_time-start_time)/60) + \"min\"\n",
    "logline = '... Finished pre-training in :' + str((end_time-start_time)/60) + \"min\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# FINETUNING THE MODEL #\n",
    "########################\n",
    "\n",
    "#get the training, validation and testing function for the model\n",
    "print '... getting the finetuning functions'\n",
    "logline = \"----Getting Finetuning-----\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "    \n",
    "train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "print '... finetuning the model'\n",
    "logline = \"------finetuning the model------\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "\n",
    "# early-stopping parameters\n",
    "print \"Train batches \" + str(n_train_batches)\n",
    "patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "patience_increase = 2.    # wait this much longer when a new best is\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "\n",
    "validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "\n",
    "while (epoch < training_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_train_batches):\n",
    "        minibatch_avg_cost = train_fn(minibatch_index)\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            validation_losses = validate_model()\n",
    "            this_validation_loss = numpy.mean(validation_losses)\n",
    "            print('epoch %i, minibatch %i/%i, validation error %f %%'\n",
    "                  % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "            logline = \"epoch: \" + str(epoch) +\"minibatch: \" + str(minibatch_index + 1) + \"validation error: \" + str(this_validation_loss * 100.)\n",
    "            f2 = open(logPickle, 'a+')\n",
    "            pickle.dump(logline , f2);\n",
    "            f2.close()\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (this_validation_loss < best_validation_loss * improvement_threshold ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "                    logline = \"epoch: \" + str(epoch) +\"minibatch: \" + str(minibatch_index + 1) + \"Test error: \" + str(test_score * 100.)\n",
    "                    f2 = open(logPickle, 'a+')\n",
    "                    pickle.dump(logline , f2);\n",
    "                    f2.close()\n",
    "\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "end_time = timeit.default_timer()\n",
    "print '... Finished fine-tuning in :' + str((end_time-start_time)/1000) + \"seconds\"\n",
    "logline = '... Finished finetuning in :' + str((end_time-start_time)/1000) + \"seconds\"\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n",
    "\n",
    "print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    ")\n",
    "logline = \"... Optimization complete with best validation socre of: \" + str(best_validation_loss * 100.) + \"in iterations: \" + str(i) + \" With test score: \" + str(test_score * 100.)\n",
    "f2 = open(logPickle, 'a+')\n",
    "pickle.dump(logline , f2);\n",
    "f2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
