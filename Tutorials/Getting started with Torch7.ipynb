{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Getting started with Torch7</h1>\n",
    "\n",
    "![](images/torch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was created by Alison B Lowndes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!  If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"timer\" src=\"timer/timer.html\" width=\"100%\" height=\"120px\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The answer should be three: 3\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"The answer should be three: \" .. (1+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thu Feb  4 14:44:10 2016       \n",
       "+------------------------------------------------------+                       \n",
       "| NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \n",
       "|-------------------------------+----------------------+----------------------+\n",
       "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
       "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
       "|===============================+======================+======================|\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\n",
       "| N/A   23C    P8    17W / 125W |     10MiB /  4095MiB |      0%      Default |\n",
       "+-------------------------------+----------------------+----------------------+\n",
       "                                                                               \n",
       "+-----------------------------------------------------------------------------+\n",
       "| Processes:                                                       GPU Memory |\n",
       "|  GPU       PID  Type  Process name                               Usage      |\n",
       "|=============================================================================|\n",
       "|  No running processes found                                                 |\n",
       "+-----------------------------------------------------------------------------+\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "true\texit\t0\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.execute(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will use Torch to give you similar information about the GPU in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'cutorch'\n",
    "print(  cutorch.getDeviceProperties(cutorch.getDevice()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Please ensure you only execute one cell at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this hands-on lab is to allow you to quickly understand Torch and its neural networks package in order to train a neural network on a GPU. If you haven’t done the other classes in the Introduction to Deep Learning course, it may be more efficient to go through the other classes first then come back and try this one. All classes and material are available at https://developer.nvidia.com/deep-learning-courses\n",
    "\n",
    "As you execute cells below, you will know the lab is processing when you see a solid (filled) circle in the top-right of the page.\n",
    "Otherwise, when it is idle, you will see the following: ![](images/iTorch.jpg)\n",
    "If a cell is stalled, you can stop it with the stop button in the toolbar.\n",
    "For troubleshooting, please see [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Torch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/torch.jpg)\n",
    "\n",
    "Torch core features include:\n",
    "\n",
    "* a powerful N-dimensional array or Tensor\n",
    "* lots of routines for indexing, slicing, transposing, ...\n",
    "* amazing interface to C, via LuaJIT\n",
    "* linear algebra routines\n",
    "* neural network, and energy-based models\n",
    "* numeric optimization routines\n",
    "* fast and efficient GPU support\n",
    "* fully embeddable, with ports to iOS, Android and FPGA backends\n",
    "\n",
    "**Why Torch?**\n",
    "The goal of Torch is for maximum flexibility and speed in building your scientific algorithms while making the process extremely simple. Torch comes with a large ecosystem of community-driven packages in machine learning, computer vision, signal processing, parallel processing, image, video, audio and networking among others, and builds on top of the Lua community.\n",
    "\n",
    "At the heart of Torch are the popular neural network and optimization libraries which are simple to use, while having maximum flexibility in implementing complex neural network topologies. You can build arbitrary graphs of neural networks, and parallelize them over CPUs and GPUs in an efficient manner.\n",
    "\n",
    "See http://torch.ch and the Cheatsheet here https://github.com/torch/torch7/wiki/Cheatsheet\n",
    "\n",
    "Torch started around 2000 with Facebook’s Ronan Collobert the main developer. Torch 7 is the current version, the 4th (using odd numbers only 1,3,5,7) aimed at web-scale learning in speech, image and video applications. Torch is used exclusively for research and prototyping for unsupervised and supervised learning, reinforcement learning, etc. Facebook, especially, spends a great deal of time improving parallelism for multi-GPU (model, data, DAG) and overlapping to improve host-device comms, as well as development in kernel speed for convolutions etc. Torch uses **automatic differentiation**. This is not numerical differentiation but a technique to take exact derivatives without needing symbolic differentation.\n",
    "\n",
    "**Maintainers**: \n",
    "* Ronan Collobert, Research Scientist @ Facebook\n",
    "* Clement Farabet, Senior Software Engineer @ Twitter\n",
    "* Koray Kavukcuoglu, Research Scientist @ Google DeepMind\n",
    "* Soumith Chintala, Research Engineer @ Facebook\n",
    "\n",
    "Torch is already used in many companies and research labs including:\n",
    "\n",
    "* Facebook AI Research\n",
    "* Google + Deepmind\n",
    "* Twitter\n",
    "* CILVR @ NYU\n",
    "* Idiap Research Institute\n",
    "* e-Lab @ Purdue\n",
    "* Element Inc\n",
    "* WhetLab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Torch core consists of the following packages:\n",
    "* torch : tensors, class factory, serialization, BLAS \n",
    "* nn : neural network Modules and Criterions\n",
    "* optim : SGD, LBFGS and other optimization functions \n",
    "* gnuplot : ploting and data visualization \n",
    "* paths : make directories, concatenate file paths, and other filesystem utilities\n",
    "* image : save, load, crop, scale, warp, translate images and such \n",
    "* trepl : the torch LuaJIT interpreter \n",
    "* cwrap : used for wrapping C/CUDA functions in Lua \n",
    "\n",
    "[LBFGS = Limited memory Broyden–Fletcher–Goldfarb–Shanno - an iterative method for solving unconstrained nonlinear optimization problems, approx. Newton’s method].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lua (JIT) \n",
    "\n",
    "Lua is a powerful, fast, lightweight, embeddable scripting language combining simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. The language is maintained by a team at PUC-Rio, the Pontifical Catholic University of Rio de Janeiro in Brazil. Lua was born and raised in Tecgraf, formerly the Computer Graphics Technology Group of PUC-Rio. Lua is now housed at LabLua, a laboratory of the Department of Computer Science of PUC-Rio.\n",
    "\n",
    "###Just In Time compilation\n",
    "Using LuaJIT allows for complex applications to be compiled and optimized but also embedded into any environment (iPhone, video games, web backends). The complete Torch framework runs on iPhone, with no mods to scripts. \n",
    "\n",
    "Torch’s universal data structure, the table, can be used as an array, a dictionary, hash table, class, struct, object, list. Torch 7 extends the table with a Tensor object, an n-dim array type.\n",
    "\n",
    "For training neural nets, autoencoders, linear regression, CNN's, RNN’s etc its all about gradients and loss functions. Torch’s nn package provides it all. \n",
    "\n",
    "Recasting a pre-defined model as a CUDA model for use on GPU’s is as simple as: model:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##LuaRocks\n",
    "Lua itself comes with a very handy manager: luarocks. \n",
    "Different demos/tutorials rely on different 3rd-party packages. If a demo crashes because it can't find a package then simply try to install it using luarocks, eg:\n",
    "\n",
    "```\n",
    "$ luarocks install image    (an image library for Torch7)\n",
    "$ luarocks install nnx      (lots of extra neural-net modules)\n",
    "```\n",
    "\n",
    "There are many many packages (or rocks) managed by LuaRocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before We Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deep learn we need data.  For the first part of this lab, w'll use the CIFAR10 dataset which is large enough to be useful, but small enough to train in a reasonable amount of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensor class is the most important class in Torch. Almost every package depends on this class - for handling numeric data. A Tensor is a serializable, potentially multi-dimensional matrix. The number of dimensions is unlimited that can be created using LongStorage.\n",
    "\n",
    "####Internal data representation\n",
    "The actual data of a Tensor is contained in a Storage. \n",
    "'Storages' are how Lua accesses memory of a C pointer or array. \n",
    "Storages can also map the contents of a file to memory. \n",
    "A Storage is an array of basic C types. \n",
    "\n",
    "Several types of Tensor exists:\n",
    "\n",
    "* ByteTensor -- contains unsigned chars\n",
    "* CharTensor -- contains signed chars\n",
    "* ShortTensor -- contains shorts\n",
    "* IntTensor -- contains ints\n",
    "* FloatTensor -- contains floats\n",
    "* DoubleTensor -- contains doubles\n",
    "\n",
    "Several Storage classes for all the basic C types exist and have the following self-explanatory names: ByteStorage, CharStorage, ShortStorage, IntStorage, LongStorage, FloatStorage, DoubleStorage. ByteStorage and CharStorage represent both arrays of bytes. \n",
    "ByteStorage represents an array of unsigned chars, while CharStorage represents an array of signed chars.\n",
    "\n",
    "**One could say that a Tensor is a particular way of viewing a Storage: a Storage only represents a chunk of memory, while the Tensor interprets this chunk of memory as having dimensions.**\n",
    "\n",
    "Let's work through some basic Torch syntax.  Execute the cells below in order and make sure you understand the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor(5,3) -- construct a 5x3 matrix, uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.rand(5,3) -- construct a 5x3 matrix with randomized data\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 1\n",
    "a*b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 2\n",
    "torch.mm(a,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 3\n",
    "c=torch.Tensor(5,4)\n",
    "c:mm(a,b) -- store the result of a*b in c\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be moved onto GPU using the :cuda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'cutorch';\n",
    "a = a:cuda()\n",
    "b = b:cuda()\n",
    "c = c:cuda()\n",
    "c:mm(a,b) -- done on GPU\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run neural networks on GPUs we use cunn: **not to be confused with cudnn**\n",
    "The nn module provides modules which each contain their state, and these modules expect CudaTensors as inputs. \n",
    "To use Cuda-based nn modules, you will need to import cunn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'cunn';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "To use GPU's with torch you call $ require \"cutorch\" on a CUDA-capable machine. \n",
    "Here's an explanation of the packages needed for using Torch with GPUs:\n",
    "\n",
    "* cutorch - Torch CUDA Implementation\n",
    "* cunn - Torch CUDA Neural Network Implementation\n",
    "* cunnx - Experimental CUDA NN implementations\n",
    "* cudnn - NVIDIA CuDNN Bindings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Neural networks in Torch can be constructed using the nn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules are the bricks used to build neural networks. Each are themselves neural networks, but can be combined with other networks using containers to create more complex neural networks.\n",
    "\n",
    "For example, LeNet, is a network that classfies digit images, a simple feed-forward network.\n",
    "\n",
    "![](images/lenet.jpg)\n",
    "\n",
    "It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "Such a network container is `nn.Sequential` which feeds the input through several layers.\n",
    "\n",
    "Let's use Torch to create a LeNet network in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(1, 6, 5, 5)) -- 1 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\n",
    "net:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)\n",
    "net:add(nn.LogSoftMax())                     -- converts the output to a log-probability. Useful for classification problems\n",
    "\n",
    "print('Lenet5\\n' .. net:__tostring());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neural network module in Torch has [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). It has a :forward(input) function that computes the output for a given input, flowing the input through the network. and it has a :backward(input, gradient) function that will differentiate each neuron in the network w.r.t. the gradient that is passed in. This is done via the [chain rule](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "Let's next use the `:forward` and `:backward` functions with the LeNet we just created on some random input data.  Excute the cells below and see if you can understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = torch.rand(1,32,32) -- pass a random tensor as input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = net:forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net:zeroGradParameters() -- zero the internal gradient buffers of the network (will come to this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradInput = net:backward(input, torch.rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#gradInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've plucked a random tensor out and given it to the network as input. Simply by passing this into the forward() function of the net it is processed - fed forward - and output as \"output\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Criterion: Defining a loss function\n",
    "When you want a model to learn to do something, you give it feedback on how well it is doing. The function that computes an objective measure of the model's performance is called a loss function and it typically takes in the model's output and the groundtruth and computes a value that quantifies the model's performance.\n",
    "The model then corrects itself to have a smaller loss.\n",
    "In Torch, loss functions are implemented just like neural network modules, and have automatic differentiation.\n",
    "They have two functions - `forward(input, target)` and `backward(input, target)`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion() -- a negative log-likelihood criterion for multi-class classification\n",
    "criterion:forward(output, 3) -- let's say the groundtruth was class number: 3\n",
    "gradients = criterion:backward(output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradInput = net:backward(input, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Recap\n",
    "Networks takes an input and produce an output in the `:forward` pass. \n",
    "'Criterion' computes the loss of the network, and its gradients with respect to the output of the network.\n",
    "Network takes an (input, gradients) pair in its backward pass and calculates the gradients with respect to each layer (and neuron) in the network.\n",
    "\n",
    "### Next\n",
    "A neural network layer can have learnable parameters or not.\n",
    "A convolution layer learns its convolution kernels to adapt to the input data and the problem being solved.\n",
    "A max-pooling layer has no learnable parameters. It only finds the max of local windows.\n",
    "A layer in Torch which has learnable weights, will typically have fields `.weight` (and optionally `.bias`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = nn.SpatialConvolution(1,3,2,2) -- learn 3 2x2 kernels\n",
    "print(m.weight) -- initially, the weights are randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(m.bias) -- The operation in a convolution layer is: output = convolution(input,weight) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two other important fields in a learnable layer. The `gradWeight` and `radBias`. \n",
    "`gradWeight` accumulates the gradients with respect to each weight in the layer, and `gradBias`, with respect to each bias in the layer.\n",
    "\n",
    "##Training the network\n",
    "For the network to adjust itself, it typically does this operation (if you do Stochastic Gradient Descent):\n",
    "\n",
    "`weight = weight + learningRate * gradWeight [equation 1]`\n",
    "\n",
    "This update over time will adjust the network weights such that the output loss is decreasing.\n",
    "\n",
    "###How does each layer in the neural network update the weight according to equation 1?\n",
    "A simple SGD trainer in the neural network module: `nn.StochasticGradient` has a function `:train(dataset)` that takes a given dataset and simply trains your network by showing different samples from your dataset to the network.\n",
    "\n",
    "####What about data?\n",
    "Torch has simple dataloaders: `image.load` or `audio.load` to load your data into a `torch.Tensor` or a Lua table.\n",
    "We'll use the CIFAR-10 dataset, which has the classes: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'.\n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel colour images of 32x32 pixels in size.\n",
    "The dataset has 50,000 training images and 10,000 test images in total.\n",
    "\n",
    "![](images/CIFAR10.jpg)\n",
    "\n",
    "We now have 5 steps left to train a Torch neural network\n",
    "1. Load and normalize data\n",
    "2. Define Neural Network\n",
    "3. Define Loss function\n",
    "4. Train network on training data\n",
    "5. Test network on test data.\n",
    "\n",
    "**1. Load and normalize data**\n",
    "In the interest of time, we prepared the data before-hand into a 4D torch ByteTensor of size 10000x3x32x32 (training) and 10000x3x32x32 (testing) Let us load the data and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset = torch.load('/home/ubuntu/data/cifar.torch/cifar10-train.t7')\n",
    "testset = torch.load('/home/ubuntu/data/cifar.torch/cifar10-test.t7')\n",
    "classes = {'airplane', 'automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#trainset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets display an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itorch.image(trainset.data[100]) -- display the 100-th image in dataset\n",
    "print(classes[trainset.label[100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the dataset to be used with **nn.StochasticGradient**, the dataset has to have a `:size()` function and an `[i]` index operator, so that `dataset[i]` returns the ith sample in the datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- ignore setmetatable for now, it is a feature beyond the scope of this tutorial. It sets the index operator.\n",
    "setmetatable(trainset, \n",
    "    {__index = function(t, i) \n",
    "                    return {t.data[i], t.label[i]} \n",
    "                end}\n",
    ");\n",
    "trainset.data = trainset.data:double() -- convert the data from a ByteTensor to a DoubleTensor.\n",
    "\n",
    "function trainset:size() \n",
    "    return self.data:size(1) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset:size()) -- just to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset[33]) -- load sample number 33.\n",
    "itorch.image(trainset[33][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One of the most important things you can do in conditioning your data (in general in data-science or machine learning) is to make your data have a mean of 0.0 and standard-deviation of 1.0.**\n",
    "This is the final step of our data processing via the tensor indexing operator. It is shown by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redChannel = trainset.data[{ {}, {1}, {}, {}  }] -- this picks {all images, 1st channel, all vertical pixels, all horizontal pixels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#redChannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this indexing operator, you initally start with [{ }]. You can pick all elements in a dimension using {} or pick a particular element using {i} where i is the element index. You can also pick a range of elements using {i1, i2}, for example {3,5} gives us the 3,4,5 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving back to mean-subtraction and standard-deviation based scaling, doing this operation is simple, using the indexing operator that we learnt above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = {} -- store the mean, to normalize the test set in the future\n",
    "stdv  = {} -- store the standard-deviation for the future\n",
    "for i=1,3 do -- over each image channel\n",
    "    mean[i] = trainset.data[{ {}, {i}, {}, {}  }]:mean() -- mean estimation\n",
    "    print('Channel ' .. i .. ', Mean: ' .. mean[i])\n",
    "    trainset.data[{ {}, {i}, {}, {}  }]:add(-mean[i]) -- mean subtraction\n",
    "    \n",
    "    stdv[i] = trainset.data[{ {}, {i}, {}, {}  }]:std() -- std estimation\n",
    "    print('Channel ' .. i .. ', Standard Deviation: ' .. stdv[i])\n",
    "    trainset.data[{ {}, {i}, {}, {}  }]:div(stdv[i]) -- std scaling\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data is now normalized and ready to be used.\n",
    "\n",
    "**2. Define our neural network**\n",
    "\n",
    "###Task 1: \n",
    "\n",
    "Modify the neural network from the Neural Networks section above and modify it to take 3-channel images (instead of 1-channel images as it was defined).\n",
    "\n",
    "You can find the [solution](#Task-#1-Answer) in the Answers section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(1, 6, 5, 5)) -- 1 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\n",
    "net:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)\n",
    "net:add(nn.LogSoftMax())                     -- converts the output to a log-probability. Useful for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define the Loss function**\n",
    "A Log-likelihood classification loss is well suited for most classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Train the neural network**\n",
    "First define an **nn.StochasticGradient** object then we'll give our dataset to this object's :train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = nn.StochasticGradient(net, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 5 -- just do 5 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer:train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Test the network, print accuracy**\n",
    "We have trained the network for 5 passes over the training dataset.\n",
    "To check if the network has learnt anything we can check by predicting the class label that the neural network outputs, and comparing it to the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Lets display an image from the test set to get familiar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classes[testset.label[100]])\n",
    "itorch.image(testset.data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize the test data with the mean and standard-deviation from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset.data = testset.data:double()   -- convert from Byte tensor to Double tensor\n",
    "for i=1,3 do -- over each image channel\n",
    "    testset.data[{ {}, {i}, {}, {}  }]:add(-mean[i]) -- mean subtraction    \n",
    "    testset.data[{ {}, {i}, {}, {}  }]:div(stdv[i]) -- std scaling\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- print the mean and standard-deviation of example-100\n",
    "horse = testset.data[100]\n",
    "print(horse:mean(), horse:std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the neural network thinks these examples above are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classes[testset.label[100]])\n",
    "itorch.image(testset.data[100])\n",
    "predicted = net:forward(testset.data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- the output of the network is Log-Probabilities. To convert them to probabilities, you have to take e^x \n",
    "print(predicted:exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the network predictions. The network assigned a probability to each class, given the image.\n",
    "To make it clearer, we can tag each probability with its class-name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,predicted:size(1) do\n",
    "    print(classes[i], predicted[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the real deal; how many in total are correct over the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i=1,10000 do\n",
    "    local groundtruth = testset.label[i]\n",
    "    local prediction = net:forward(testset.data[i])\n",
    "    local confidences, indices = torch.sort(prediction, true)  -- true means sort in descending order\n",
    "    if groundtruth == indices[1] then\n",
    "        correct = correct + 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(correct, 100*correct/10000 .. ' % ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which classes performed well, and which didn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_performance = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}\n",
    "for i=1,10000 do\n",
    "    local groundtruth = testset.label[i]\n",
    "    local prediction = net:forward(testset.data[i])\n",
    "    local confidences, indices = torch.sort(prediction, true)  -- true means sort in descending order\n",
    "    if groundtruth == indices[1] then\n",
    "        class_performance[groundtruth] = class_performance[groundtruth] + 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,#classes do\n",
    "    print(classes[i], 100*class_performance[i]/1000 .. ' %')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To swap from CPU to GPU we simply take a neural network, and transfer it over to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'cunn'; \n",
    "--brings in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = net:cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = criterion:cuda()\n",
    "-- transfer the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset.data = trainset.data:cuda()\n",
    "-- transfer the data across"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- and train on GPU \n",
    "trainer = nn.StochasticGradient(net, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 5 -- just do 5 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer:train(trainset)\n",
    "-- this is such a small dataset you wont notice much difference on speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To try another dataset with a simple script ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clement Farabet of Twitter lets you run a full network here on Google Street View House Numbers dataset. Due to time we've set the -size flag to small (uses 10,000 training images only of the 73,000+)\n",
    "\n",
    " ![](images/SVHN.jpg)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "----------------------------------------------------------------------\n",
    "-- This script loads the (SVHN) House Numbers dataset\n",
    "-- http://ufldl.stanford.edu/housenumbers/\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "-- Note: files were converted from their original Matlab format\n",
    "-- to Torch's internal format using the mattorch package. The\n",
    "-- mattorch package allows 1-to-1 conversion between Torch and Matlab\n",
    "-- files.\n",
    "\n",
    "-- The SVHN dataset contains 3 files:\n",
    "--    + train: training data\n",
    "--    + test:  test data\n",
    "--    + extra: extra training data\n",
    "\n",
    "train_file = '/home/ubuntu/data/svhn/housenumbers/train_32x32.t7'\n",
    "test_file = '/home/ubuntu/data/svhn/housenumbers/test_32x32.t7'\n",
    "extra_file = '/home/ubuntu/data/svhn/housenumbers/extra_32x32.t7'\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> loading dataset'\n",
    "\n",
    "-- We load the dataset from disk, and re-arrange it to be compatible\n",
    "-- with Torch's representation. Matlab uses a column-major representation,\n",
    "-- Torch is row-major, so we just have to transpose the data.\n",
    "\n",
    "-- Note: the data, in X, is 4-d: the 1st dim indexes the samples, the 2nd\n",
    "-- dim indexes the color channels (RGB), and the last two dims index the\n",
    "-- height and width of the samples.\n",
    "\n",
    "loaded = torch.load(train_file,'ascii')\n",
    "trainData = {\n",
    "   data = loaded.X:transpose(3,4),\n",
    "   labels = loaded.y[1],\n",
    "   size = function() return trsize end\n",
    "}\n",
    "\n",
    "loaded = torch.load(extra_file,'ascii')\n",
    "extraTrainData = {\n",
    "   data = loaded.X:transpose(3,4),\n",
    "   labels = loaded.y[1],\n",
    "   size = function() return trsize end\n",
    "}\n",
    "\n",
    "loaded = torch.load(test_file,'ascii')\n",
    "testData = {\n",
    "   data = loaded.X:transpose(3,4),\n",
    "   labels = loaded.y[1],\n",
    "   size = function() return tesize end\n",
    "}\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> visualizing data'\n",
    "\n",
    "-- Visualization is quite easy, using itorch.image().\n",
    "if itorch then\n",
    "   print('training data:')\n",
    "   itorch.image(trainData.data[{ {1,128} }])\n",
    "   print('extra training data:')\n",
    "   itorch.image(extraTrainData.data[{ {1,128} }])\n",
    "   print('test data:')\n",
    "   itorch.image(testData.data[{ {1,128} }])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "----------------------------------------------------------------------\n",
    "-- This tutorial shows how to train different models on the street\n",
    "-- view house number dataset (SVHN),\n",
    "-- using multiple optimization techniques (SGD, ASGD, CG), and\n",
    "-- multiple types of models.\n",
    "--\n",
    "-- This script demonstrates a classical example of training \n",
    "-- well-known models (convnet, MLP, logistic regression)\n",
    "-- on a 10-class classification problem. \n",
    "--\n",
    "-- It illustrates several points:\n",
    "-- 1/ description of the model\n",
    "-- 2/ choice of a loss function (criterion) to minimize\n",
    "-- 3/ creation of a dataset as a simple Lua table\n",
    "-- 4/ description of training and test procedures\n",
    "--\n",
    "-- Clement Farabet\n",
    "----------------------------------------------------------------------\n",
    "require 'torch'\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> processing options'\n",
    "\n",
    "cmd = torch.CmdLine()\n",
    "cmd:text()\n",
    "cmd:text('SVHN Loss Function')\n",
    "cmd:text()\n",
    "cmd:text('Options:')\n",
    "-- global:\n",
    "cmd:option('-seed', 1, 'fixed input seed for repeatable experiments')\n",
    "cmd:option('-threads', 2, 'number of threads')\n",
    "-- data:\n",
    "cmd:option('-size', 'small', 'how many samples do we load: small | full | extra')\n",
    "-- model:\n",
    "cmd:option('-model', 'convnet', 'type of model to construct: linear | mlp | convnet')\n",
    "-- loss:\n",
    "cmd:option('-loss', 'nll', 'type of loss function to minimize: nll | mse | margin')\n",
    "-- training:\n",
    "cmd:option('-save', 'results', 'subdirectory to save/log experiments in')\n",
    "cmd:option('-plot', false, 'live plot')\n",
    "cmd:option('-optimization', 'SGD', 'optimization method: SGD | ASGD | CG | LBFGS')\n",
    "cmd:option('-learningRate', 1e-3, 'learning rate at t=0')\n",
    "cmd:option('-batchSize', 1, 'mini-batch size (1 = pure stochastic)')\n",
    "cmd:option('-weightDecay', 0, 'weight decay (SGD only)')\n",
    "cmd:option('-momentum', 0, 'momentum (SGD only)')\n",
    "cmd:option('-t0', 1, 'start averaging at t0 (ASGD only), in nb of epochs')\n",
    "cmd:option('-maxIter', 2, 'maximum nb of iterations for CG and LBFGS')\n",
    "cmd:option('-type', 'double', 'type: double | float | cuda')\n",
    "cmd:text()\n",
    "opt = cmd:parse(arg or {})\n",
    "\n",
    "-- nb of threads and fixed seed (for repeatable experiments)\n",
    "if opt.type == 'float' then\n",
    "   print('==> switching to floats')\n",
    "   torch.setdefaulttensortype('torch.FloatTensor')\n",
    "elseif opt.type == 'cuda' then\n",
    "   print('==> switching to CUDA')\n",
    "   require 'cunn'\n",
    "   torch.setdefaulttensortype('torch.FloatTensor')\n",
    "end\n",
    "torch.setnumthreads(opt.threads)\n",
    "torch.manualSeed(opt.seed)\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> executing all'\n",
    "\n",
    "dofile '/home/ubuntu/data/svhn/1_data.lua'\n",
    "dofile '/home/ubuntu/data/svhn/2_model.lua'\n",
    "dofile '/home/ubuntu/data/svhn/3_loss.lua'\n",
    "dofile '/home/ubuntu/data/svhn/4_train.lua'\n",
    "dofile '/home/ubuntu/data/svhn/5_test.lua'\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> training!'\n",
    "\n",
    "while true do\n",
    "   train()\n",
    "   test()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our developers are working hard to offer full integration on DIGITS, our deep learning GPU training system and will be releasing a beta shortly. Here is the output of training Lenet on MNIST using Torch - the screen shots looks very much like Caffe but this is raw Torch output.\n",
    "\n",
    "![](images/digits.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, please go to **your browsers File menu** (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "Torch is maintained by the deep learning field's top coders and this lab is thanks to them. They are all very happy to offer this assistance because they want you using their highly optimized framework, **Torch7**. \n",
    "\n",
    "The Cheatsheet https://github.com/torch/torch7/wiki/Cheatsheet and all code are maintained on Github https://github.com/torch/torch7 but if you do have **advanced questions ONLY** you can head over to Gitter where most devs hang out, but PLEASE - for install or newbie questions go to the Google Group first for very quick responses.\n",
    "* Ask for help: http://groups.google.com/forum/#!forum/torch7\n",
    "* Chat with developers of Torch: http://gitter.im/torch/torch7\n",
    "\n",
    "\n",
    "### More information\n",
    "\n",
    "iTorch is simply an iPython kernel for Torch allowing images, video etc and you can use it outside of this lab instance via https://github.com/facebook/iTorch#requirements\n",
    "\n",
    "For more advanced coding you need to download and try Torch yourself. Torch is open-source, so you can also start with the code on the GitHub repo https://github.com/torch/torch7 and use the Getting Started guide here http://torch.ch/docs/getting-started.html. You can learn more about LuaJIT here http://luajit.org/\n",
    "\n",
    "* Build crazy graphs of networks: https://github.com/torch/nngraph\n",
    "* Train on imagenet with multiple GPUs: https://github.com/soumith/imagenet-multiGPU.torch\n",
    "* Train recurrent networks with LSTM on text: https://github.com/wojzaremba/lstm\n",
    "* More demos and tutorials: https://github.com/torch/torch7/wiki/Cheatsheet\n",
    "\n",
    "Due to Torch's embeddable nature it can even be run on our NVIDIA Jetson TK1 boards.\n",
    "Installation and usage instructions for Torch + CuDNN on Jetson TK1 is here:\n",
    "https://github.com/e-lab/torch-toolbox/blob/master/Tutorials/Setup-Torch-cuDNN-on-Jetson-TK1.md \n",
    "\n",
    "For Matlab users, UCLA’s Ata Mahjoubfar, PhD has kindly written a separate very thorough cheatsheet for you here: http://atamahjoubfar.github.io/Torch_for_Matlab_users.pdf\n",
    "\n",
    "For Numpy users there’s https://github.com/torch/torch7/wiki/Torch-for-Numpy-users\n",
    "\n",
    "For advanced users, please refer to the “gotchas” here https://luapower.com/luajit-notes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about these other topics, please visit:\n",
    "* GPU accelerated machine learning: [http://www.nvidia.com/object/machine-learning.html](http://www.nvidia.com/object/machine-learning.html)\n",
    "* Theano: [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)\n",
    "* Torch: [http://torch.ch/](http://torch.ch/)\n",
    "* DIGITS: [https://developer.nvidia.com/digits](https://developer.nvidia.com/digits)\n",
    "* cuDNN: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\n",
    "\n",
    "### Deep Learning Lab Series\n",
    "\n",
    "Make sure to check out the rest of the classes in this Deep Learning lab series.  You can find them [here](https://developer.nvidia.com/deep-learning-courses).\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "Many thanks to \n",
    "* Soumith Chintala of Facebook for his 2015 **60 minute blitz** on Github https://github.com/soumith/cvpr2015/blob/master/Deep%20Learning%20with%20Torch.ipynb\n",
    "* Clement Farabet for his Madbits tutorials here http://code.madbits.com/wiki/doku.php\n",
    "* Mark Ebersole and Larry Brown @NVIDIA for their help putting together this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "### Task #1 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(3, 6, 5, 5)) -- 1 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\n",
    "net:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)\n",
    "net:add(nn.LogSoftMax())                     -- converts the output to a log-probability. Useful for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Task #1](#Task-1:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
